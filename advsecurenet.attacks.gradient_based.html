<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>advsecurenet.attacks.gradient_based package &mdash; AdvSecureNet  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=5929fcd5"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="advsecurenet.dataloader package" href="advsecurenet.dataloader.html" />
    <link rel="prev" title="advsecurenet.attacks.decision_based package" href="advsecurenet.attacks.decision_based.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            AdvSecureNet
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">AdvSecureNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">How to Contribute?</a></li>
<li class="toctree-l1"><a class="reference internal" href="attacks.html">Adversarial Attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="defenses.html">Adversarial Defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluations.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AdvSecureNet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="api.html">API Documentation</a></li>
          <li class="breadcrumb-item"><a href="advsecurenet.html">AdvSecureNet Package</a></li>
          <li class="breadcrumb-item"><a href="advsecurenet.attacks.html">advsecurenet.attacks package</a></li>
      <li class="breadcrumb-item active">advsecurenet.attacks.gradient_based package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/advsecurenet.attacks.gradient_based.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="advsecurenet-attacks-gradient-based-package">
<h1>advsecurenet.attacks.gradient_based package<a class="headerlink" href="#advsecurenet-attacks-gradient-based-package" title="Link to this heading"></a></h1>
<hr class="docutils" />
<section id="module-advsecurenet.attacks.gradient_based.cw">
<span id="advsecurenet-attacks-gradient-based-cw-module"></span><h2>advsecurenet.attacks.gradient_based.cw module<a class="headerlink" href="#module-advsecurenet.attacks.gradient_based.cw" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.cw.CWAttack">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">advsecurenet.attacks.gradient_based.cw.</span></span><span class="sig-name descname"><span class="pre">CWAttack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.shared.types.configs.attack_configs.html#advsecurenet.shared.types.configs.attack_configs.cw_attack_config.CWAttackConfig" title="advsecurenet.shared.types.configs.attack_configs.cw_attack_config.CWAttackConfig"><span class="pre">CWAttackConfig</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.cw.CWAttack" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="advsecurenet.attacks.base.html#advsecurenet.attacks.base.adversarial_attack.AdversarialAttack" title="advsecurenet.attacks.base.adversarial_attack.AdversarialAttack"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdversarialAttack</span></code></a></p>
<p>Carlini-Wagner L2 attack</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>targeted</strong> (<em>bool</em>) – If True, targets the attack to the specified label. Defaults to False.</p></li>
<li><p><strong>c_init</strong> (<em>float</em>) – Initial value of c. Defaults to 0.1.</p></li>
<li><p><strong>kappa</strong> (<em>float</em>) – Confidence parameter for CW loss. Defaults to 0.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – Learning rate for the Adam optimizer. Defaults to 0.01.</p></li>
<li><p><strong>max_iterations</strong> (<em>int</em>) – Maximum number of iterations for the Adam optimizer. Defaults to 10.</p></li>
<li><p><strong>abort_early</strong> (<em>bool</em>) – If True, aborts the attack early if the loss stops decreasing. Defaults to False.</p></li>
<li><p><strong>binary_search_steps</strong> (<em>int</em>) – Number of binary search steps. Defaults to 10.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – Device to use for the attack. Defaults to “cpu”.</p></li>
<li><p><strong>clip_min</strong> (<em>float</em>) – Minimum value of the input. Defaults to 0.</p></li>
<li><p><strong>clip_max</strong> (<em>float</em>) – Maximum value of the input. Defaults to 1.</p></li>
<li><p><strong>c_lower</strong> (<em>float</em>) – Lower bound for c. Defaults to 1e-6.</p></li>
<li><p><strong>c_upper</strong> (<em>float</em>) – Upper bound for c. Defaults to 1.</p></li>
<li><p><strong>patience</strong> (<em>int</em>) – Number of iterations to wait before aborting early. Defaults to 5.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – If True, prints progress of the attack. Defaults to True.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<p>[1] Carlini, Nicholas, and David Wagner. “Towards evaluating the robustness of neural networks.” 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.</p>
<dl class="py method">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.cw.CWAttack.attack">
<span class="sig-name descname"><span class="pre">attack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><span class="pre">BaseModel</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.cw.CWAttack.attack" title="Link to this definition"></a></dt>
<dd><p>Performs the Carlini-Wagner L2 attack on the specified model and input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><em>BaseModel</em></a>) – Model to attack.</p></li>
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Batch of inputs to attack.</p></li>
<li><p><strong>y</strong> (<em>torch.Tensor</em>) – Label of the input. If targeted is True, the attack will try to make the model predict this label. Otherwise, the attack will try to make the model predict any other label than this one.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Adversarial example.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-advsecurenet.attacks.gradient_based.deepfool">
<span id="advsecurenet-attacks-gradient-based-deepfool-module"></span><h2>advsecurenet.attacks.gradient_based.deepfool module<a class="headerlink" href="#module-advsecurenet.attacks.gradient_based.deepfool" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.deepfool.DeepFool">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">advsecurenet.attacks.gradient_based.deepfool.</span></span><span class="sig-name descname"><span class="pre">DeepFool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.shared.types.configs.attack_configs.html#advsecurenet.shared.types.configs.attack_configs.deepfool_attack_config.DeepFoolAttackConfig" title="advsecurenet.shared.types.configs.attack_configs.deepfool_attack_config.DeepFoolAttackConfig"><span class="pre">DeepFoolAttackConfig</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.deepfool.DeepFool" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="advsecurenet.attacks.base.html#advsecurenet.attacks.base.adversarial_attack.AdversarialAttack" title="advsecurenet.attacks.base.adversarial_attack.AdversarialAttack"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdversarialAttack</span></code></a></p>
<p>DeepFool attack</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of classes in the dataset. Defaults to 10.</p></li>
<li><p><strong>overshoot</strong> (<em>float</em>) – Overshoot parameter. Defaults to 0.02.</p></li>
<li><p><strong>max_iterations</strong> (<em>int</em>) – Maximum number of iterations. Defaults to 50.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – Device to use for the attack. Defaults to “cpu”.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<p>[1] Moosavi-Dezfooli, Seyed-Mohsen, et al. “Deepfool: a simple and accurate method to fool deep neural networks.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</p>
<dl class="py method">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.deepfool.DeepFool.attack">
<span class="sig-name descname"><span class="pre">attack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><span class="pre">BaseModel</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tensor</span></span></span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.deepfool.DeepFool.attack" title="Link to this definition"></a></dt>
<dd><p>Generates adversarial examples using the DeepFool attack.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><em>BaseModel</em></a>) – The model to attack.</p></li>
<li><p><strong>x</strong> (<em>torch.tensor</em>) – The original input tensor. Expected shape is (batch_size, channels, height, width).</p></li>
<li><p><strong>y</strong> (<em>torch.tensor</em>) – The true labels for the input tensor. Expected shape is (batch_size,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The adversarial example tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-advsecurenet.attacks.gradient_based.fgsm">
<span id="advsecurenet-attacks-gradient-based-fgsm-module"></span><h2>advsecurenet.attacks.gradient_based.fgsm module<a class="headerlink" href="#module-advsecurenet.attacks.gradient_based.fgsm" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.fgsm.FGSM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">advsecurenet.attacks.gradient_based.fgsm.</span></span><span class="sig-name descname"><span class="pre">FGSM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.shared.types.configs.attack_configs.html#advsecurenet.shared.types.configs.attack_configs.fgsm_attack_config.FgsmAttackConfig" title="advsecurenet.shared.types.configs.attack_configs.fgsm_attack_config.FgsmAttackConfig"><span class="pre">FgsmAttackConfig</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.fgsm.FGSM" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="advsecurenet.attacks.base.html#advsecurenet.attacks.base.adversarial_attack.AdversarialAttack" title="advsecurenet.attacks.base.adversarial_attack.AdversarialAttack"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdversarialAttack</span></code></a></p>
<p>Fast Gradient Sign Method attack. The attack can be targeted or untargeted.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epsilon</strong> (<em>float</em>) – The epsilon value to use for the attack. Defaults to 0.3.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – Device to use for the attack. Defaults to “cpu”.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<p>[1] Goodfellow, Ian J., et al. “Explaining and harnessing adversarial examples.” arXiv preprint arXiv:1412.6572 (2014).</p>
<dl class="py method">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.fgsm.FGSM.attack">
<span class="sig-name descname"><span class="pre">attack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><span class="pre">BaseModel</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.fgsm.FGSM.attack" title="Link to this definition"></a></dt>
<dd><p>Generates adversarial examples using the FGSM attack.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><em>BaseModel</em></a>) – The model to attack.</p></li>
<li><p><strong>x</strong> (<em>torch.tensor</em>) – The original input tensor. Expected shape is (batch_size, channels, height, width).</p></li>
<li><p><strong>y</strong> (<em>torch.tensor</em>) – If the attack is targeted, the target labels tensor. Else, the original labels tensor. Expected shape is (batch_size,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The adversarial example tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-advsecurenet.attacks.gradient_based.lots">
<span id="advsecurenet-attacks-gradient-based-lots-module"></span><h2>advsecurenet.attacks.gradient_based.lots module<a class="headerlink" href="#module-advsecurenet.attacks.gradient_based.lots" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.lots.LOTS">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">advsecurenet.attacks.gradient_based.lots.</span></span><span class="sig-name descname"><span class="pre">LOTS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.shared.types.configs.attack_configs.html#advsecurenet.shared.types.configs.attack_configs.lots_attack_config.LotsAttackConfig" title="advsecurenet.shared.types.configs.attack_configs.lots_attack_config.LotsAttackConfig"><span class="pre">LotsAttackConfig</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.lots.LOTS" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="advsecurenet.attacks.base.html#advsecurenet.attacks.base.adversarial_attack.AdversarialAttack" title="advsecurenet.attacks.base.adversarial_attack.AdversarialAttack"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdversarialAttack</span></code></a></p>
<p>LOTS attack</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>deep_feature_layer</strong> (<em>str</em>) – The name of the layer to use for the attack.</p></li>
<li><p><strong>mode</strong> (<a class="reference internal" href="advsecurenet.shared.types.configs.attack_configs.html#advsecurenet.shared.types.configs.attack_configs.lots_attack_config.LotsAttackMode" title="advsecurenet.shared.types.configs.attack_configs.lots_attack_config.LotsAttackMode"><em>LotsAttackMode</em></a>) – The mode to use for the attack. Defaults to LotsAttackMode.ITERATIVE.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – The epsilon value to use for the attack. Defaults to 0.1.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – The learning rate to use for the attack. Defaults to 1./255.</p></li>
<li><p><strong>max_iterations</strong> (<em>int</em>) – The maximum number of iterations to use for the attack. Defaults to 1000.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – Whether to print progress of the attack. Defaults to True.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – Device to use for the attack. Defaults to “cpu”.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<p>[1] Rozsa, A., Güunther, M., and Boult, T. E. (2017). LOTS about attacking deep features. In International Joint Conference on Biometrics (IJCB), pages 168{176. IEEE. <a class="reference external" href="https://arxiv.org/abs/1611.06179">https://arxiv.org/abs/1611.06179</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.lots.LOTS.attack">
<span class="sig-name descname"><span class="pre">attack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><span class="pre">BaseModel</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="advsecurenet.dataloader.html#advsecurenet.dataloader.data_loader_factory.DataLoaderFactory.None" title="advsecurenet.dataloader.data_loader_factory.DataLoaderFactory.None"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.lots.LOTS.attack" title="Link to this definition"></a></dt>
<dd><p>Generates adversarial examples using the LOTS attack. Based on the provided mode, either the iterative or single attack will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><em>BaseModel</em></a>) – The model to attack.</p></li>
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – The original input tensor. Shape: (batch_size, channels, height, width).</p></li>
<li><p><strong>x_target</strong> (<em>torch.Tensor</em>) – The x_target tensor. Shape: (batch_size, channels, height, width).</p></li>
<li><p><strong>y</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – The target classes tensor. Shape: (batch_size,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The adversarial example tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-advsecurenet.attacks.gradient_based.pgd">
<span id="advsecurenet-attacks-gradient-based-pgd-module"></span><h2>advsecurenet.attacks.gradient_based.pgd module<a class="headerlink" href="#module-advsecurenet.attacks.gradient_based.pgd" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.pgd.PGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">advsecurenet.attacks.gradient_based.pgd.</span></span><span class="sig-name descname"><span class="pre">PGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.shared.types.configs.attack_configs.html#advsecurenet.shared.types.configs.attack_configs.pgd_attack_config.PgdAttackConfig" title="advsecurenet.shared.types.configs.attack_configs.pgd_attack_config.PgdAttackConfig"><span class="pre">PgdAttackConfig</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.pgd.PGD" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="advsecurenet.attacks.base.html#advsecurenet.attacks.base.adversarial_attack.AdversarialAttack" title="advsecurenet.attacks.base.adversarial_attack.AdversarialAttack"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdversarialAttack</span></code></a></p>
<p>Projected Gradient Descent targeted / untargeted attack using l-infinity norm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epsilon</strong> (<em>float</em>) – The epsilon value to use for the attack. Defaults to 0.3.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – The alpha value to use for the attack. Defaults to 2/255.</p></li>
<li><p><strong>num_iter</strong> (<em>int</em>) – The number of iterations to use for the attack. Defaults to 40.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – Device to use for the attack. Defaults to “cpu”.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<p>[1] Madry, Aleksander, et al. “Towards deep learning models resistant to adversarial attacks.” arXiv preprint arXiv:1706.06083 (2017).</p>
<dl class="py method">
<dt class="sig sig-object py" id="advsecurenet.attacks.gradient_based.pgd.PGD.attack">
<span class="sig-name descname"><span class="pre">attack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><span class="pre">BaseModel</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#advsecurenet.attacks.gradient_based.pgd.PGD.attack" title="Link to this definition"></a></dt>
<dd><p>Performs the PGD attack on the specified model and input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="advsecurenet.models.html#advsecurenet.models.base_model.BaseModel" title="advsecurenet.models.base_model.BaseModel"><em>BaseModel</em></a>) – The model to attack.</p></li>
<li><p><strong>x</strong> (<em>torch.tensor</em>) – The original input tensor. Expected shape is (batch_size, channels, height, width).</p></li>
<li><p><strong>y</strong> (<em>torch.tensor</em>) – The true labels for the input tensor. Expected shape is (batch_size,).</p></li>
<li><p><strong>targeted</strong> (<em>bool</em>) – If True, targets the attack to the specified label. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The adversarial example tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<hr class="docutils" />
<span class="target" id="module-advsecurenet.attacks.gradient_based"></span></section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="advsecurenet.attacks.decision_based.html" class="btn btn-neutral float-left" title="advsecurenet.attacks.decision_based package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="advsecurenet.dataloader.html" class="btn btn-neutral float-right" title="advsecurenet.dataloader package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Melih Catal.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>