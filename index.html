

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AdvSecureNet Documentation &mdash; AdvSecureNet  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="AdvSecureNet" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            AdvSecureNet
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">AdvSecureNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">How to Contribute?</a></li>
<li class="toctree-l1"><a class="reference internal" href="attacks.html">Adversarial Attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="defenses.html">Adversarial Defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluations.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">AdvSecureNet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">AdvSecureNet Documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="advsecurenet-documentation">
<h1>AdvSecureNet Documentation<a class="headerlink" href="#advsecurenet-documentation" title="Link to this heading"></a></h1>
</section>
<section id="advsecurenet">
<h1>AdvSecureNet<a class="headerlink" href="#advsecurenet" title="Link to this heading"></a></h1>
<section id="build-and-ci-cd-status">
<h2>Build and CI/CD Status<a class="headerlink" href="#build-and-ci-cd-status" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://github.com/melihcatal/advsecurenet/actions/workflows/python-ci.yml"><img alt="Unit Tests and Style Checks" src="https://github.com/melihcatal/advsecurenet/actions/workflows/python-ci.yml/badge.svg?branch=develop" /></a> <a class="reference external" href="https://github.com/melihcatal/advsecurenet/actions/workflows/documentation.yml"><img alt="Build and Deploy Sphinx Documentation" src="https://github.com/melihcatal/advsecurenet/actions/workflows/documentation.yml/badge.svg" /></a> <a class="reference external" href="https://github.com/melihcatal/advsecurenet/actions/workflows/python-publish.yml"><img alt="Upload Python Package" src="https://github.com/melihcatal/advsecurenet/actions/workflows/python-publish.yml/badge.svg" /></a></p>
</section>
<section id="code-quality-and-coverage">
<h2>Code Quality and Coverage<a class="headerlink" href="#code-quality-and-coverage" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Quality Gate Status" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=alert_status" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Bugs" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=bugs" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Code Smells" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=code_smells" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Coverage" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=coverage" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Duplicated Lines (%)" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=duplicated_lines_density" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Lines of Code" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=ncloc" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Reliability Rating" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=reliability_rating" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Security Rating" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=security_rating" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Technical Debt" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=sqale_index" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Maintainability Rating" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=sqale_rating" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Vulnerabilities" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=vulnerabilities" /></a></p>
</section>
<section id="package-information">
<h2>Package Information<a class="headerlink" href="#package-information" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://github.com/psf/black"><img alt="Code Style: Black" src="https://img.shields.io/badge/code%20style-black-000000.svg" /></a> <a class="reference external" href="https://pypi.org/project/advsecurenet/"><img alt="pypi" src="https://img.shields.io/pypi/v/advsecurenet" /></a> <a class="reference external" href="https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow"><img alt="gitflow" src="https://img.shields.io/badge/git-flow-blue" /></a></p>
<p align="center"></p><p>AdvSecureNet is a Python library for Machine Learning Security,
developed by <a class="reference external" href="https://github.com/melihcatal">Melih Catal</a> at
<a class="reference external" href="https://www.uzh.ch/en.html">University of Zurich</a> as part of his
Master’s Thesis under the supervision of <a class="reference external" href="https://www.ifi.uzh.ch/en/aiml/people/guenther.html">Prof. Dr. Manuel
Günther</a>. The
main focus of the library is on adversarial attacks and defenses for
vision tasks, with plans to extend support to other tasks such as
natural language processing.</p>
<p>The library provides tools to generate adversarial examples, evaluate
the robustness of machine learning models against adversarial attacks,
and train robust machine learning models. Built on top of
<a class="reference external" href="https://pytorch.org/">PyTorch</a>, it is designed to be modular and
extensible, making it easy to run experiments with different
configurations. AdvSecureNet supports multi-GPU setups to enhance
computational efficiency and fully supports both CLI and API interfaces,
along with external YAML configuration files, enabling comprehensive
testing and evaluation, facilitating the sharing and reproducibility of
experiments.</p>
<section id="features">
<h3>Features<a class="headerlink" href="#features" title="Link to this heading"></a></h3>
<p><strong>Adversarial Attacks:</strong> AdvSecureNet supports a diverse range of
evasion attacks on computer vision tasks, including gradient-based,
decision-based, single-step, iterative, white-box, black-box, targeted,
and untargeted attacks, enabling comprehensive testing and evaluation of
neural network robustness against various types of adversarial examples.</p>
<p><strong>Adversarial Defenses:</strong> The toolkit includes adversarial training and
ensemble adversarial training. Adversarial training incorporates
adversarial examples into the training process to improve model
robustness, while ensemble adversarial training uses multiple models or
attacks for a more resilient defense strategy.</p>
<p><strong>Evaluation Metrics:</strong> AdvSecureNet supports metrics like accuracy,
robustness, transferability, and similarity. Accuracy measures
performance on clean data, robustness assesses resistance to attacks,
transferability evaluates how well adversarial examples deceive
different models, and similarity quantifies perceptual differences using
PSNR and SSIM.</p>
<p><strong>Multi-GPU Support:</strong> AdvSecureNet is optimized for multi-GPU setups,
enhancing the efficiency of training, evaluation, and adversarial attack
generation, especially for large models and datasets or complex methods.
By utilizing multiple GPUs in parallel, AdvSecureNet aims to reduce
computational time, making it ideal for large-scale experiments and deep
learning models.</p>
<p><strong>CLI and API Interfaces:</strong> AdvSecureNet offers both CLI and API
interfaces. The CLI allows for quick execution of attacks, defenses, and
evaluations, while the API provides advanced integration and extension
within user applications.</p>
<p><strong>External Configuration Files:</strong> The toolkit supports YAML
configuration files for easy parameter tuning and experimentation. This
feature enables users to share experiments, reproduce results, and
manage setups effectively, facilitating collaboration and comparison.</p>
<p><strong>Built-in Models and Datasets Support:</strong> AdvSecureNet supports all
PyTorch vision library models and well-known datasets like CIFAR-10,
CIFAR-100, MNIST, FashionMNIST, and SVHN. Users can start without
additional setup, but the toolkit also allows for custom datasets and
models, offering flexibility for various research and applications.</p>
<p><strong>Automated Adversarial Target Generation:</strong> AdvSecureNet can
automatically generate adversarial targets for targeted attacks,
simplifying the process and ensuring consistent and reliable results. As
a user, you don’t need to manually specify targets. This feature is
especially useful for targeted attacks on large datasets. You can also
provide custom targets if you prefer.</p>
</section>
<section id="supported-attacks">
<h3>Supported Attacks<a class="headerlink" href="#supported-attacks" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1412.6572">FGSM - FGSM Targeted</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1706.06083">PGD - PGD Targeted</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1511.04599">DeepFool</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1608.04644">CW</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1611.06179">LOTS</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1712.04248">Decision Boundary</a></p></li>
</ul>
</section>
<section id="supported-defenses">
<h3>Supported Defenses<a class="headerlink" href="#supported-defenses" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1412.6572">Adversarial Training</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1705.07204">Ensemble Adversarial Training</a></p></li>
</ul>
</section>
<section id="supported-evaluation-metrics">
<h3>Supported Evaluation Metrics<a class="headerlink" href="#supported-evaluation-metrics" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Benign Accuracy</p></li>
<li><p>Attack Success Rate</p></li>
<li><p>Transferability</p></li>
<li><p>Perturbation Distance</p></li>
<li><p>Robustness Gap</p></li>
<li><p>Perturbation Effectiveness</p></li>
</ul>
</section>
</section>
<section id="similarity-metrics">
<h2>Similarity Metrics<a class="headerlink" href="#similarity-metrics" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">PSNR - Peak Signal-to-Noise
Ratio</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Structural_similarity">SSIM - Structural Similarity
Index</a></p></li>
</ul>
<section id="why-advsecurenet">
<h3>Why AdvSecureNet?<a class="headerlink" href="#why-advsecurenet" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Research-Oriented</strong>: Easily run and share experiments with
different configurations using YAML configuration files.</p></li>
<li><p><strong>Supports Various Attacks and Defenses</strong>: Experiment with a wide
range of adversarial attacks and defenses.</p></li>
<li><p><strong>Supports Any PyTorch Model</strong>: Use pre-trained models or your own
PyTorch models with the library.</p></li>
<li><p><strong>Supports Various Evaluation Metrics</strong>: Evaluate the robustness of
models, performance of adversarial attacks, and defenses.</p></li>
<li><p><strong>Benign Use Case Support</strong>: Train and evaluate models on benign
data.</p></li>
<li><p><strong>Native Multi-GPU Support</strong>: Efficiently run large-scale experiments
utilizing multiple GPUs.</p></li>
<li><p><strong>CLI and API Support</strong>: Use the command line interface for quick
experiments or the Python API for advanced integration.</p></li>
<li><p><strong>Automated Adversarial Target Generation</strong>: Simplify targeted
attacks by letting the library generate targets automatically.</p></li>
<li><p><strong>Active Maintenance</strong>: Regular updates and improvements to ensure
the library remains relevant and useful.</p></li>
<li><p><strong>Comprehensive Documentation</strong>: Detailed documentation to help you
get started and make the most of the library.</p></li>
<li><p><strong>Open Source</strong>: Free and open-source under the MIT license, allowing
you to use, modify, and distribute the library.</p></li>
</ul>
</section>
<section id="comparison-with-other-libraries">
<h3>Comparison with Other Libraries<a class="headerlink" href="#comparison-with-other-libraries" title="Link to this heading"></a></h3>
<p>AdvSecureNet stands out among adversarial machine learning toolkits like
IBM ART, AdverTorch, SecML, FoolBox, ARES, and CleverHans. Key
advantages include:</p>
<ul class="simple">
<li><p><strong>Active Maintenance:</strong> Ensures ongoing support and updates.</p></li>
<li><p><strong>Comprehensive Training Support:</strong> One of the few toolkits supporting both adversarial and ensemble adversarial training.</p></li>
<li><p><strong>Multi-GPU Support:</strong> The first toolkit with native multi-GPU support for attacks, defenses, and evaluations, ideal for large-scale experiments.</p></li>
<li><p><strong>Flexible Interfaces:</strong> The first toolkit that fully supports CLI, API usage, and external YAML configuration files for reproducibility for all features.</p></li>
<li><p><strong>Performance:</strong> AdvSecureNet excels in performance, significantly reducing execution times on multi-GPU setups.</p></li>
</ul>
<p><img alt="comparison_table" src="https://github.com/melihcatal/advsecurenet/assets/46859098/0e0498dc-d3ea-41c4-a634-b4df5bbb0ce1" />
<img alt="performance_comparision" src="https://github.com/melihcatal/advsecurenet/assets/46859098/e88658d3-1852-43ed-8c37-530e7fb7a0ce" /></p>
<p>[1] SecML supports attacks from CleverHans and FoolBox [2] This feature
is only available for adversarial training.</p>
</section>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h3>
<p>You can install the library using <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>advsecurenet
</pre></div>
</div>
<p>Or install it from source:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/melihcatal/advsecurenet.git
<span class="nb">cd</span><span class="w"> </span>advsecurenet
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
</section>
<section id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h3>
<p>The library can be used as a command line tool or as an importable
Python package.</p>
</section>
</section>
<section id="command-line-tool">
<h2>Command Line Tool<a class="headerlink" href="#command-line-tool" title="Link to this heading"></a></h2>
<p>Use the <code class="docutils literal notranslate"><span class="pre">advsecurenet</span></code> command to interact with the library. Use
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span> <span class="pre">--help</span></code> to see available commands and options. It is
recommended to use YAML configuration files to run experiments. You can
list the available configuration options using
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span> <span class="pre">utils</span> <span class="pre">configs</span> <span class="pre">list</span></code> and generate a template
configuration file using
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span> <span class="pre">utils</span> <span class="pre">configs</span> <span class="pre">get</span> <span class="pre">-c</span> <span class="pre">&lt;config_name&gt;</span> <span class="pre">-o</span> <span class="pre">&lt;output_file&gt;</span></code>.</p>
<p>Running an adversarial attack:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>advsecurenet<span class="w"> </span>attack<span class="w"> </span>-c<span class="w"> </span>./fgsm.yml
</pre></div>
</div>
<p>Running an adversarial defense:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>advsecurenet<span class="w"> </span>defense<span class="w"> </span>adversarial-training<span class="w"> </span>-c<span class="w"> </span>./adv_training.yml
</pre></div>
</div>
<p>Running an evaluation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>advsecurenet<span class="w"> </span>evaluate<span class="w"> </span>benign<span class="w"> </span>-c<span class="w"> </span>./evaluate_benign.yml

or

advsecurenet<span class="w"> </span>evaluate<span class="w"> </span>adversarial<span class="w"> </span>-c<span class="w"> </span>./evaluate_adversarial.yml
</pre></div>
</div>
</section>
<section id="python-package">
<h2>Python Package<a class="headerlink" href="#python-package" title="Link to this heading"></a></h2>
<p>You can import the library as a Python package. You can use the
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span></code> module to access the library. You can find the
available modules and classes in the
<a class="reference external" href="http://melihcatal.github.io/advsecurenet/">documentation</a>.</p>
<p><img alt="image" src="https://github.com/melihcatal/advsecurenet/assets/46859098/f3f86817-8ac3-4523-8f5e-cc9d4b4cbcf3" /> <em>Usage example of AdvSecureNet demonstrating the equivalence
between a YAML configuration file with a command-line interface (CLI)
command and a corresponding Python API implementation.</em></p>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h3>
<p>Examples of different use cases can be found in the
<a class="reference external" href="./examples/">examples</a> directory.</p>
</section>
<section id="architecture">
<h3>Architecture<a class="headerlink" href="#architecture" title="Link to this heading"></a></h3>
<p>The high-level architecture of the toolkit is shown in the figure below.</p>
<figure class="align-default" id="id66">
<img alt="advsecurenet_arch" src="https://drive.switch.ch/index.php/s/SdKAyOZs1d9bcin/download" />
<figcaption>
<p><span class="caption-text">advsecurenet_arch</span><a class="headerlink" href="#id66" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-default" id="id67">
<img alt="cli-arch" src="https://drive.switch.ch/index.php/s/ZbjIHBHql0dV6n0/download" />
<figcaption>
<p><span class="caption-text">cli-arch</span><a class="headerlink" href="#id67" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The toolkit is designed to be modular and extensible. CLI and Python API
are implemented separately, however, they share the same core components
and they have the same package structure for the sake of consistency.
Tests are implemented for both CLI and Python API to ensure the
correctness of the implementation and again they follow the same
structure. The toolkit is designed to be easily extensible, new attacks,
defenses, and evaluation metrics can be added by implementing the
corresponding classes and registering them in the corresponding
registries.</p>
</section>
<section id="testing">
<h3>Testing<a class="headerlink" href="#testing" title="Link to this heading"></a></h3>
<p>The library is tested using <code class="docutils literal notranslate"><span class="pre">pytest</span></code> and coverage is measured using
<code class="docutils literal notranslate"><span class="pre">coverage</span></code>. You can run the tests using the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>tests/
</pre></div>
</div>
<p>Some tests take longer to run. To speed up the tests, you can use the
<code class="docutils literal notranslate"><span class="pre">--device</span></code> option to run tests on a specific device (e.g.,
<code class="docutils literal notranslate"><span class="pre">--device</span> <span class="pre">cuda:0</span></code>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>tests/<span class="w"> </span>--device<span class="w"> </span>cuda:0
</pre></div>
</div>
<p>Tests are categorized into the following groups:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cli:</span></code> tests for the command line interface</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">advsecurenet:</span></code> tests for the Python API</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">essential:</span></code> tests for essential functionality (e.g., smoke and
unit tests)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">comprehensive:</span></code> tests for comprehensive functionality (e.g.,
integration tests)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">extended:</span></code> tests for extended functionality (e.g., performance
tests, security tests)</p></li>
</ul>
<p>You can run tests for a specific group using the <code class="docutils literal notranslate"><span class="pre">m</span></code> option and the
group name. For example, to run tests for the CLI:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>tests/<span class="w"> </span>-m<span class="w"> </span>cli
</pre></div>
</div>
<p>CI/CD pipelines are set up to run tests automatically on every push and
pull request. You can see the status of the tests in the badges at the
top of the README.</p>
</section>
<section id="quality-assurance">
<h3>Quality Assurance<a class="headerlink" href="#quality-assurance" title="Link to this heading"></a></h3>
<p>AdvSecureNet is designed with a strong emphasis on code quality and
maintainability. The toolkit follows best practices in software
engineering and ensures high standards through the following measures:</p>
<ul class="simple">
<li><p><strong>PEP 8 Compliance</strong>: The codebase adheres to PEP 8 guidelines, the
de facto coding standard for Python. We use
<a class="reference external" href="https://github.com/psf/black">Black</a> for automatic code
formatting to maintain consistent style and readability.</p></li>
<li><p><strong>Static Code Analysis</strong>: We employ
<a class="reference external" href="https://www.pylint.org/">Pylint</a> for static code analysis and
<a class="reference external" href="http://mypy-lang.org/">MyPy</a> for type checking. These tools help
catch potential errors and enforce coding standards.</p></li>
<li><p><strong>Code Quality and Complexity</strong>: Tools like
<a class="reference external" href="https://www.sonarqube.org/">SonarQube</a> and
<a class="reference external" href="https://radon.readthedocs.io/">Radon</a> provide insights into code
quality and complexity. These tools are integrated into our CI/CD
pipelines to ensure that the code remains clean and maintainable.</p></li>
<li><p><strong>Comprehensive Testing</strong>: The project features a robust testing
suite, ensuring that all components are thoroughly tested. This helps
in maintaining the reliability and stability of the toolkit.</p></li>
<li><p><strong>Continuous Integration/Continuous Deployment (CI/CD)</strong>: CI/CD
pipelines are set up to automate the testing, analysis, and
deployment processes. This ensures that any changes to the codebase
are automatically verified for quality and correctness before being
merged.</p></li>
<li><p><strong>Documentation</strong>: Comprehensive documentation is available on
<a class="reference external" href="https://melihcatal.github.io/advsecurenet/">GitHub Pages</a>,
providing detailed guidance on installation, usage, and API
references. This ensures that users and contributors can easily
understand and work with the toolkit.</p></li>
</ul>
<p>By adhering to these practices and leveraging these tools, AdvSecureNet
maintains a high standard of code quality, ensuring a reliable and
user-friendly experience for developers and researchers alike.</p>
</section>
<section id="license">
<h3>License<a class="headerlink" href="#license" title="Link to this heading"></a></h3>
<p>This project is licensed under the terms of the MIT license. See
<a class="reference external" href="./LICENSE">LICENSE</a> for more details.</p>
</section>
</section>
</section>
<section id="how-to-contribute">
<h1>How to Contribute?<a class="headerlink" href="#how-to-contribute" title="Link to this heading"></a></h1>
<p>Thank you for considering contributing to <code class="docutils literal notranslate"><span class="pre">AdvSecureNet</span></code>. We welcome
all contributions, including new features, bug fixes, and documentation
improvements. To make the process seamless, we have prepared a guide for
different types of contributions.</p>
<p>If you find a bug in the project, please open an issue on the GitHub
repository. When reporting a bug, include the following details:</p>
<ul class="simple">
<li><p>A detailed description of the issue.</p></li>
<li><p>Steps to reproduce the issue.</p></li>
<li><p>The version of the project you are using.</p></li>
<li><p>The operating system you are running.</p></li>
<li><p>The Python version.</p></li>
<li><p>If applicable:</p>
<ul>
<li><p>The CUDA version.</p></li>
</ul>
</li>
<li><p>Any other relevant information.</p></li>
</ul>
<p>An example of a good bug report would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Description</span><span class="p">:</span> <span class="p">[</span><span class="n">Detailed</span> <span class="n">description</span> <span class="n">of</span> <span class="n">the</span> <span class="n">issue</span><span class="p">]</span>

<span class="n">Steps</span> <span class="n">to</span> <span class="n">Reproduce</span><span class="p">:</span>

<span class="mf">1.</span>  <span class="p">[</span><span class="n">Step</span> <span class="n">one</span><span class="p">]</span>
<span class="mf">2.</span>  <span class="p">[</span><span class="n">Step</span> <span class="n">two</span><span class="p">]</span>
<span class="mf">3.</span>  <span class="p">[</span><span class="n">Step</span> <span class="n">three</span><span class="p">]</span>

<span class="n">Version</span><span class="p">:</span> <span class="p">[</span><span class="n">Project</span> <span class="n">version</span><span class="p">]</span>
<span class="n">OS</span><span class="p">:</span> <span class="p">[</span><span class="n">Operating</span> <span class="n">system</span><span class="p">]</span>
<span class="n">Python</span><span class="p">:</span> <span class="p">[</span><span class="n">Python</span> <span class="n">version</span> <span class="p">]</span>
<span class="n">CUDA</span><span class="p">:</span> <span class="p">[</span><span class="n">CUDA</span> <span class="n">version</span> <span class="k">if</span> <span class="n">applicable</span><span class="p">]</span>
<span class="n">Other</span><span class="p">:</span> <span class="p">[</span><span class="n">Any</span> <span class="n">other</span> <span class="n">relevant</span> <span class="n">information</span><span class="p">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">AdvSecureNet</span></code> has two main components: the API and the CLI. You can
contribute to either component. The high-level process is the same:</p>
<ol class="arabic simple">
<li><p>Fork the repository.</p></li>
<li><p>Create a new branch for your changes.</p></li>
<li><p>Make your changes.</p></li>
<li><p>Write tests for your changes.</p></li>
<li><p>Run the tests.</p></li>
<li><p>Document your changes.</p></li>
<li><p>Submit a pull request.</p></li>
<li><p>Stay tuned for feedback.</p></li>
</ol>
<section id="code-quality-standards">
<h2>Code Quality Standards<a class="headerlink" href="#code-quality-standards" title="Link to this heading"></a></h2>
<p>To ensure code quality, follow these standards:</p>
<ul class="simple">
<li><p>Format code using <code class="docutils literal notranslate"><span class="pre">black</span></code>.</p></li>
<li><p>Lint code using <code class="docutils literal notranslate"><span class="pre">pylint</span></code>.</p></li>
<li><p>Test code using <code class="docutils literal notranslate"><span class="pre">pytest</span></code>.</p></li>
<li><p>Document code using <code class="docutils literal notranslate"><span class="pre">sphinx</span></code>.</p></li>
<li><p>Type-check code using <code class="docutils literal notranslate"><span class="pre">mypy</span></code>.</p></li>
<li><p>Have your code reviewed by at least one other contributor before
merging.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">AdvSecureNet</span></code> uses <code class="docutils literal notranslate"><span class="pre">Gitflow</span></code> as its branching model:</p>
<ul class="simple">
<li><p>Develop new features in a <code class="docutils literal notranslate"><span class="pre">feature</span></code> branch.</p></li>
<li><p>Develop bug fixes in a <code class="docutils literal notranslate"><span class="pre">hotfix</span></code> branch.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">main</span></code> branch is reserved for stable releases.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch is used for development.</p></li>
</ul>
<p>When submitting a pull request, target the <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch. For more
information on <code class="docutils literal notranslate"><span class="pre">Gitflow</span></code>, refer to <a class="reference external" href="https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow">this
guide</a>.</p>
</section>
<section id="creating-a-new-feature">
<h2>Creating a New Feature<a class="headerlink" href="#creating-a-new-feature" title="Link to this heading"></a></h2>
<p>You have three options for creating a new feature:</p>
<ul class="simple">
<li><p>A new attack</p></li>
<li><p>A new defense</p></li>
<li><p>A new evaluation metric / evaluator</p></li>
</ul>
<p>Depending on the feature type and the target component (API or CLI), the
process will differ. Refer to the following sections for more
information.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">advsecurenet</span></code> package contains an <code class="docutils literal notranslate"><span class="pre">attacks</span></code> module with various
submodules based on attack types (e.g., gradient-based, decision-based).
If your attack does not fit into any existing submodule, feel free to
create a new one. Currently, <code class="docutils literal notranslate"><span class="pre">AdvSecureNet</span></code> supports evasion attacks
on computer vision models only. All attacks should inherit from the
<code class="docutils literal notranslate"><span class="pre">AdversarialAttack</span></code> class, an abstract base class that defines the
interface for all attacks. The <code class="docutils literal notranslate"><span class="pre">AdversarialAttack</span></code> class is defined in
the <code class="docutils literal notranslate"><span class="pre">attacks.base</span></code> module. Additionally, each attack should have its
own configuration class, which defines the parameters of the attack.
This approach keeps the attack class clean and makes it easier to use
the attack in the CLI. The configuration class should be defined in the
<code class="docutils literal notranslate"><span class="pre">shared.types.configs.attack_configs</span></code> folder and should inherit from
the <code class="docutils literal notranslate"><span class="pre">AttackConfig</span></code> class. The <code class="docutils literal notranslate"><span class="pre">AttackConfig</span></code> class, also defined in
the <code class="docutils literal notranslate"><span class="pre">shared.types.configs.attack_configs</span></code> folder, contains the
<code class="docutils literal notranslate"><span class="pre">device</span></code> attribute, specifying the device on which the attack should
run, and a flag indicating whether the attack is targeted or untargeted.</p>
<p>Follow these steps to create a new attack:</p>
<ol class="arabic simple">
<li><p>Create a new submodule in the <code class="docutils literal notranslate"><span class="pre">attacks</span></code> module.</p></li>
<li><p>Create a class for your attack that inherits from the
<code class="docutils literal notranslate"><span class="pre">AdversarialAttack</span></code> class.</p></li>
<li><p>Create a configuration class for your attack that inherits from the
<code class="docutils literal notranslate"><span class="pre">AttackConfig</span></code> class.</p></li>
<li><p>Implement the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, accepting the configuration as an
argument.</p></li>
<li><p>Implement the <code class="docutils literal notranslate"><span class="pre">attack</span></code> method, taking the model, input, and target
as arguments and returning the adversarial example.</p></li>
</ol>
<p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">advsecurenet.attacks.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdversarialAttack</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">advsecurenet.shared.types.configs.attack_configs</span><span class="w"> </span><span class="kn">import</span> <span class="n">AttackConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">advsecurenet.models.base_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>

<span class="nd">@dataclass</span><span class="p">(</span><span class="n">kw_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">RandomNoiseAttackConfig</span><span class="p">(</span><span class="n">AttackConfig</span><span class="p">):</span>
    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RandomNoiseAttack</span><span class="p">(</span><span class="n">AdversarialAttack</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RandomNoiseAttackConfig</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">epsilon</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">attack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>
        <span class="k">return</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">noise</span>
</pre></div>
</div>
<p>Using a dataclass for the configuration class makes it easy to create
instances of the class with default values. It also prevents users from
passing invalid arguments to the attack. Using <code class="docutils literal notranslate"><span class="pre">kw_only=True</span></code> ensures
that users have to pass the arguments by keyword, which makes the code
more readable and less error-prone. Additionally, it facilitates the
future extension of the configuration class with new parameters without
breaking the existing code.</p>
<p>To use your attack in the CLI, follow these additional steps:</p>
<ol class="arabic simple">
<li><p>Create Default YAML Configuration Files</p></li>
</ol>
<p>1.1. For Adversarial Training Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>•   Create a configuration file for the attack parameters in the `cli/configs/attacks/base` folder.
•   Create another configuration file for the attack itself (including other necessary parameters) in the cli/configs/attacks folder.
</pre></div>
</div>
<p>This separation makes it easier to use the attack both in the
adversarial training command and as a standalone attack.</p>
<p>Example of a configuration file for the attack parameters:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Description: Base configuration file for the attack parameters. Located in the cli/configs/attacks/base folder</span>
<span class="nt">target_parameters</span><span class="p">:</span><span class="w"> </span><span class="kt">!include</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../shared/attack_target_config.yml</span><span class="w"> </span><span class="c1"># Targeted attack configs</span>
<span class="nt">attack_parameters</span><span class="p">:</span>
<span class="nt">epsilon</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.3</span><span class="w"> </span><span class="c1"># The epsilon value to be used for the FGSM attack. The higher the value, the more the perturbation</span>
</pre></div>
</div>
<p>Example of a configuration file for the attack:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Description: Configuration file for the attack. Located in the cli/configs/attacks folder</span>

<span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="kt">!include</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../shared/model_config.yml</span>
<span class="nt">dataset</span><span class="p">:</span><span class="w"> </span><span class="kt">!include</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./shared/attack_dataset_config.yml</span>
<span class="nt">dataloader</span><span class="p">:</span><span class="w"> </span><span class="kt">!include</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../shared/dataloader_config.yml</span>
<span class="nt">device</span><span class="p">:</span><span class="w"> </span><span class="kt">!include</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../shared/device_config.yml</span>
<span class="nt">attack_procedure</span><span class="p">:</span><span class="w"> </span><span class="kt">!include</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./shared/attack_procedure_config.yml</span>

<span class="c1"># Attack Specific Configuration</span>
<span class="nt">attack_config</span><span class="p">:</span><span class="w"> </span><span class="kt">!include</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./base/attack_base_config.yml</span>
</pre></div>
</div>
<p>The first file contains the parameters specific to the attack, while the
second file contains the parameters common to all attacks. The first
file is used when another command wants to use the attack as a
parameter, which means that command takes care of the essential
parameters like the model, dataset, etc. The second file is used when
the attack is used as a standalone command, which means that the attack
command needs to configure the necessary parameters to prepare the
environment for the attack and then run the attack.</p>
<ol class="arabic simple" start="2">
<li><p>Create a configuration dataclass for your attack in the
<code class="docutils literal notranslate"><span class="pre">cli/shared/types/attack/attacks</span></code> folder.</p></li>
<li><p>Update the attack mapping in the
<code class="docutils literal notranslate"><span class="pre">cli/shared/utils/attack_mappings.py</span></code> file to include your attack.</p></li>
<li><p>Finally, update the <code class="docutils literal notranslate"><span class="pre">cli/commands/attack/commands.py</span></code> module to
include your attack as a subcommand.</p></li>
</ol>
<p id="api-1">There is no base class for defenses in <code class="docutils literal notranslate"><span class="pre">AdvSecureNet</span></code>. However, each
defense should have its own configuration class similar to attacks.
Follow these steps:</p>
<ol class="arabic simple">
<li><p>Create a new submodule in the <code class="docutils literal notranslate"><span class="pre">defenses</span></code> module.</p></li>
<li><p>Create a configuration class for your defense.</p></li>
<li><p>Create a class for your defense and use the configuration class you
created to initialize the defense in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p></li>
</ol>
<p id="cli-1">Since there isn’t a <code class="docutils literal notranslate"><span class="pre">defender</span></code> module to run the defenses, unlike the
attacks, you need to create the logic for the defense in the
<code class="docutils literal notranslate"><span class="pre">cli/logic/defense</span></code> folder. To add your defense to the CLI:</p>
<ol class="arabic simple">
<li><p>Create a default YAML configuration file for your defense in the
<code class="docutils literal notranslate"><span class="pre">cli/configs/defenses</span></code> folder.</p></li>
<li><p>Create a configuration dataclass for your defense in the
<code class="docutils literal notranslate"><span class="pre">cli/shared/types/defense/defenses</span></code> folder.</p></li>
<li><p>Implement the defense logic in the <code class="docutils literal notranslate"><span class="pre">cli/logic/defense</span></code> folder.</p></li>
<li><p>Add the defense to the <code class="docutils literal notranslate"><span class="pre">cli/commands/defense/commands.py</span></code> module.</p></li>
</ol>
<p id="api-2">Evaluation metrics are used to assess the performance of attacks,
defenses or models. The <code class="docutils literal notranslate"><span class="pre">advsecurenet</span></code> package includes an
<code class="docutils literal notranslate"><span class="pre">evaluation</span></code> module that contains all the evaluation metrics. Each
evaluation metric should inherit from the <code class="docutils literal notranslate"><span class="pre">BaseEvaluator</span></code> class, an
abstract base class that defines the interface for all evaluation
metrics. The evaluators are context managers, allowing them to be used
in a <code class="docutils literal notranslate"><span class="pre">with</span></code> statement to automatically clean up any resources they
use. The <code class="docutils literal notranslate"><span class="pre">BaseEvaluator</span></code> class is defined in the
<code class="docutils literal notranslate"><span class="pre">evaluation.base_evaluator</span></code> module.</p>
<ol class="arabic simple">
<li><p>Create a new class for your evaluation metric that inherits from the
<code class="docutils literal notranslate"><span class="pre">BaseEvaluator</span></code> class in the <code class="docutils literal notranslate"><span class="pre">evaluation.evaluators</span></code> folder.</p></li>
<li><p>Implement the <code class="docutils literal notranslate"><span class="pre">update</span></code> method of your evaluation metric class. This
method defines how the evaluation metric should be updated when a new
sample is evaluated.</p></li>
<li><p>Implement the <code class="docutils literal notranslate"><span class="pre">get_results</span></code> method of your evaluation metric class.
This method should return the final result of the evaluation metric.</p></li>
<li><p>If the evaluator is an adversarial evaluator, update the
<code class="docutils literal notranslate"><span class="pre">advsecurenet.shared.adversarial_evaluators</span></code> module to include your
evaluator.</p></li>
</ol>
<p id="cli-2">Evaluation metrics are automatically available in the CLI once the API
is updated. No additional steps are needed. This is because the CLI uses
the attacker to run evaluation metrics, and they are not run
independently.</p>
<section id="documentation">
<h3>Documentation<a class="headerlink" href="#documentation" title="Link to this heading"></a></h3>
<p>Improving documentation is always appreciated. If you find any part of
the codebase that is not well-documented or could be improved, please
open a pull request with your changes. We value any help in making the
documentation more comprehensive and easier to understand.</p>
</section>
</section>
</section>
<section id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading"></a></h1>
<p>Attack Success Rate (ASR) is the fundamental metric for evaluating the adversarial robustness of the model. It is the percentage of the adversarial examples that are successfully misclassified by the model. In the context of adversarial attacks, success can be defined differently for targeted and untargeted attacks. For targeted attacks, success means that the adversarial example is classified as a specific target class. For untargeted attacks, success means that the adversarial example is classified as any class other than the true class. The higher the ASR, the more vulnerable the model is to adversarial attacks. The ASR is given by the equation below.</p>
<div class="math notranslate nohighlight">
\[ASR = \frac{\text{Number of Successful Adversarial Examples}}{\text{Total Number of Adversarial Examples}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><strong>Number of Successful Adversarial Examples</strong> refers to the count of adversarial examples that lead to a successful attack, where success is defined as:
- In targeted attacks, the adversarial example is classified as the target class, <em>y:sub:`target`</em>.
- In untargeted attacks, the adversarial example is classified as any class other than the true class, <em>y:sub:`true`</em>.</p></li>
<li><p><strong>Total Number of Adversarial Examples</strong> is the total number of adversarial examples generated from inputs where the model initially makes a correct prediction.</p></li>
</ul>
<p>The evaluation of adversarial transferability is facilitated by the <code class="docutils literal notranslate"><span class="pre">transferability_evaluator</span></code> class within the <code class="docutils literal notranslate"><span class="pre">advsecurenet.evaluation.evaluators</span></code> module. This evaluator is designed to assess the effectiveness of adversarial examples, originally generated for a source model, in deceiving various target models.</p>
<p>The evaluator is initialized with a list of target models. During the evaluation phase, the <code class="docutils literal notranslate"><span class="pre">update</span></code> method calculates whether the adversarial examples successfully mislead both the source and the target models. Success in targeted attacks is determined by the adversarial example being classified as the target class, while in untargeted attacks, success is achieved if the example is classified as any class other than its true class.</p>
<p>The evaluator maintains a tally of successful deceptions for each target model, as well as the total count of adversarial examples that successfully deceive the source model. The transferability rate for each target model is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[Transferability\ Rate = \frac{\text{Number of Successful Transfers to Target Model}}{\text{Total Number of Successful Adversarial Examples on Source Model}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><strong>Number of Successful Transfers to Target Model</strong> is the count of adversarial examples that successfully deceive the target model.</p></li>
<li><p><strong>Total Number of Successful Adversarial Examples on Source Model</strong> refers to the count of adversarial examples that initially misled the source model.</p></li>
</ul>
<p>This evaluation method provides a thorough analysis of the transferability of adversarial examples across different models, shedding light on the robustness of each model against such attacks.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Consider a scenario with a source model, Model_A, and two target models, Model_B and Model_C. Suppose Model_A generates 100 adversarial examples, out of which 80 successfully deceive Model_A. When these 80 adversarial examples are tested against Model_B, 50 are successful, and against Model_C, 30 are successful.</p>
<p>Using the transferability rate formula:</p>
<p>For Model_B:</p>
<div class="math notranslate nohighlight">
\[Transferability\ Rate_{Model_B} = \frac{50}{80} = 0.625\]</div>
<p>For Model_C:</p>
<div class="math notranslate nohighlight">
\[Transferability\ Rate_{Model_C} = \frac{30}{80} = 0.375\]</div>
<p>This indicates that adversarial examples from Model_A are more transferable to Model_B than to Model_C.</p>
</div>
<p>Robustness Gap is a metric that measures the difference between the accuracy on clean examples and the accuracy on adversarial examples. The higher the robustness gap is, the more vulnerable the model is to adversarial attacks. Possible values for the robustness gap are between 0 and 1. 0 means that the model performs the same on clean and adversarial examples. 1 means that the model performs perfectly on clean examples but completely fails on adversarial examples.</p>
<p><strong>Clean Accuracy</strong> (A<sub>clean</sub>) is calculated as the ratio of the total number of correctly classified clean images (N<sub>correct_clean</sub>) to the total number of samples (N<sub>total</sub>):</p>
<div class="math notranslate nohighlight">
\[A_{\text{clean}} = \frac{N_{\text{correct_clean}}}{N_{\text{total}}}\]</div>
<p><strong>Adversarial Accuracy</strong> (A<sub>adv</sub>) is the ratio of the total number of correctly classified adversarial images (N<sub>correct_adv</sub>) to the total number of samples (N<sub>total</sub>):</p>
<div class="math notranslate nohighlight">
\[A_{\text{adv}} = \frac{N_{\text{correct_adv}}}{N_{\text{total}}}\]</div>
<p><strong>The Robustness Gap</strong> (G<sub>robust</sub>) is the difference between Clean Accuracy and Adversarial Accuracy:</p>
<div class="math notranslate nohighlight">
\[G_{\text{robust}} = A_{\text{clean}} - A_{\text{adv}}\]</div>
<p>Evaluator for the perturbation effectiveness. The effectiveness score is the attack success rate divided by the perturbation distance. The higher the score, the more effective the attack.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Suppose we have a model tested on a dataset of 1000 images. Out of these, the model correctly classifies 950 clean images, giving us N<sub>correct_clean</sub> = 950 and N<sub>total</sub> = 1000. When exposed to adversarial examples, the model correctly classifies only 700 of these images, thus N<sub>correct_adv</sub> = 700.</p>
<p>Using the provided formulas, we can calculate the Clean Accuracy (A<sub>clean</sub>) and the Adversarial Accuracy (A<sub>adv</sub>):</p>
<div class="math notranslate nohighlight">
\[A_{\text{clean}} = \frac{950}{1000} = 0.95\]</div>
<div class="math notranslate nohighlight">
\[A_{\text{adv}} = \frac{700}{1000} = 0.70\]</div>
<p>Then, the Robustness Gap (G<sub>robust</sub>) is calculated as:</p>
<div class="math notranslate nohighlight">
\[G_{\text{robust}} = A_{\text{clean}} - A_{\text{adv}} = 0.95 - 0.70 = 0.25\]</div>
<p>This Robustness Gap of 0.25 indicates that the model’s performance significantly degrades when exposed to adversarial examples, revealing a vulnerability to such attacks.</p>
</div>
<p>Perturbation Effectiveness is a metric for evaluating the effectiveness of the adversarial perturbation. It is the percentage of the adversarial perturbation that is effective in changing the model’s prediction. The higher the perturbation effectiveness is, the more effective the adversarial perturbation is. The purpose of this metric is to distinguish between attacks that have a high success rate but require a large perturbation magnitude, and attacks that have a lower success rate but require a smaller perturbation magnitude. The perturbation effectiveness is given by the equation below.</p>
<div class="math notranslate nohighlight">
\[PE = \frac{\text{Attack Success Rate}}{\text{Perturbation}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><strong>Attack Success Rate</strong> is the percentage of the adversarial examples that are successfully misclassified by the model.</p></li>
<li><p><strong>Perturbation</strong> is the perturbation magnitude of the adversarial examples. It can be measured using different norms, such as L1, L2, or Linf.</p></li>
</ul>
<p>The Peak Signal-to-Noise Ratio (PSNR) metric is a standard used in the field of image processing for assessing the quality of reconstructed or compressed images in relation to the original ones. The PSNR is derived from the mean squared error (MSE) between the original image and the reconstructed one. It is typically expressed in decibels (dB), indicating the ratio of the maximum possible power of a signal to the power of corrupting noise.</p>
<p>The formula for PSNR is given by:</p>
<div class="math notranslate nohighlight">
\[\text{PSNR} = 10 \cdot \log_{10} \left( \frac{\text{MAX}_I^2}{\text{MSE}} \right)\]</div>
<p>where MAX_I represents the maximum possible pixel value of the image (e.g., 255 for 8-bit images), and MSE is the mean squared error between the original and reconstructed images.</p>
<p>The range of PSNR is typically between 0 dB to infinity, with higher values indicating a smaller difference between the original and reconstructed image, and thus, better quality. In cases where the original and reconstructed images are identical, the MSE becomes zero, leading to an undefined PSNR in the logarithmic scale, which can be theoretically considered as infinite. A higher PSNR value generally suggests that the reconstructed image closely resembles the original image in quality.</p>
<p>The Structural Similarity Index Measure (SSIM) is a metric used for measuring the similarity between two images. Unlike traditional methods like PSNR that focus on pixel-level differences, SSIM considers changes in structural information, luminance, and contrast, providing a more perceptually relevant assessment of image quality.</p>
<p>The formula for SSIM is given by:</p>
<div class="math notranslate nohighlight">
\[\text{SSIM}(x, y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are the two images being compared, <span class="math notranslate nohighlight">\(\mu_x\)</span>, <span class="math notranslate nohighlight">\(\mu_y\)</span> are the average pixel values, <span class="math notranslate nohighlight">\(\sigma_x^2\)</span>, <span class="math notranslate nohighlight">\(\sigma_y^2\)</span> are the variances, <span class="math notranslate nohighlight">\(\sigma_{xy}\)</span> is the covariance of the images, and <span class="math notranslate nohighlight">\(C_1\)</span>, <span class="math notranslate nohighlight">\(C_2\)</span> are constants used to stabilize the division.</p>
<p>The SSIM index is a decimal value between -1 and 1, where a value of 1 for SSIM implies no difference between the compared images. As the value decreases, the differences between the images increase. SSIM is particularly useful in contexts where a human observer’s assessment of quality is important, as it aligns more closely with human visual perception than metrics based solely on pixel differences, which makes it suitable for adversarial robustness evaluation.</p>
<p><img alt="ssim_psnr" src="https://github.com/melihcatal/advsecurenet/assets/46859098/ac7344d0-cef0-4ec3-b784-9aec32a0c80d" /> <em>Comparison of SSIM and PSNR Metrics vs. Epsilon in FGSM Attack</em></p>
<p><img alt="ssim_psnr_example" src="https://github.com/melihcatal/advsecurenet/assets/46859098/b8e3d753-5bf5-4963-9f1b-b36df3acae5d" /> <em>SSIM and PSNR Example. Taken from https://medium.com/&#64;datamonsters/a-quick-overview-of-methods-to-measure-the-similarity-between-images-f907166694ee</em></p>
</section>
<section id="adversarial-attacks">
<h1>Adversarial Attacks<a class="headerlink" href="#adversarial-attacks" title="Link to this heading"></a></h1>
<p>AdvSecureNet supports various adversarial attacks, including:</p>
<ul class="simple">
<li><p>Fast Gradient Sign Method (FGSM)</p></li>
<li><p>Carlini and Wagner (C&amp;W)</p></li>
<li><p>Projected Gradient Descent (PGD)</p></li>
<li><p>DeepFool</p></li>
<li><p>Decision Boundary</p></li>
<li><p>Layerwise Origin-Target Synthesis (LOTS)</p></li>
</ul>
<p>Some of these attacks are targeted, while others are untargeted.
AdvSecureNet provides a simple way to use targeted adversarial attacks
by having an automatic target generation mechanism. This mechanism
generates a target label that is different from the original label of
the input image. The target label is chosen randomly from the set of
possible labels, excluding the original label. This ensures that the
attack is targeted, as the goal is to mislead the model into predicting
the target label instead of the correct one without being explicitly
specified by the user. This feature is particularly useful for the large
datasets where manually specifying target labels for each input image is
impractical. However, users can also specify the target label manually
if they would like to do so.</p>
<p>Below, we provide a brief overview of each adversarial attack supported
by AdvSecureNet, including its characteristics, purpose, and potential
applications.</p>
<p>Adversarial attacks can be categorized in different ways. One way to
categorize them is based on the <strong>information</strong> they use. Broadly, these
attacks fall into two categories: <strong>white-box</strong> and <strong>black-box</strong>
attacks <a href="#id68"><span class="problematic" id="id1">[1]_</span></a> <a href="#id69"><span class="problematic" id="id2">[2]_</span></a>. White-box attacks necessitate access to the model’s
parameters, making them intrinsically reliant on detailed knowledge of
the model’s internals <a href="#id70"><span class="problematic" id="id3">[3]_</span></a> <a href="#id71"><span class="problematic" id="id4">[4]_</span></a>. In contrast, black-box attacks operate
without requiring access to the model’s parameters <a href="#id72"><span class="problematic" id="id5">[5]_</span></a> <a href="#id73"><span class="problematic" id="id6">[6]_</span></a>. Among
black-box attack methods, one prevalent approach involves training a
substitute model to exploit the transferability of adversarial
attacks <a href="#id74"><span class="problematic" id="id7">[7]_</span></a>, targeting the victim model indirectly <a href="#id75"><span class="problematic" id="id8">[8]_</span></a>. Additionally,
there are other black-box attack methods such as decision boundary
attacks <a class="footnote-reference brackets" href="#id31" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> and zeroth order optimization based attacks such as
ZOO <a class="footnote-reference brackets" href="#id32" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. These methods, distinct from the substitute model approach,
rely solely on the output of the model, further reinforcing their
classification as black-box attacks.</p>
<p>Adversarial attacks can also be differentiated based on the <strong>number of
steps</strong> involved in generating adversarial perturbations. This
categorization divides them into <strong>single-step</strong> and <strong>iterative</strong>
attacks <a class="footnote-reference brackets" href="#id33" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>. Single-step attacks are characterized by their speed, as
they require only one step to calculate the adversarial perturbation. On
the other hand, iterative attacks are more time-consuming, involving
multiple steps to incrementally compute the adversarial
perturbation <a class="footnote-reference brackets" href="#id34" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>.</p>
<p>Additionally, another classification of adversarial attacks hinges on
the <strong>objective of the attack</strong>. In this context, attacks are grouped
into <strong>targeted</strong> and <strong>untargeted</strong> categories <a class="footnote-reference brackets" href="#id35" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>. Targeted attacks
are designed with the specific goal of manipulating the model’s output
to a predetermined class. In contrast, untargeted attacks are aimed at
causing the model to incorrectly classify the input into any class,
provided it is not the correct one <a class="footnote-reference brackets" href="#id36" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>.</p>
<hr class="docutils" />
<section id="fgsm">
<h2>FGSM<a class="headerlink" href="#fgsm" title="Link to this heading"></a></h2>
<p>FGSM, short for Fast Gradient Sign Method, is a type of adversarial
attack that was introduced by Goodfellow et al. <a class="footnote-reference brackets" href="#id37" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a> in 2015. It is a
single-step, white box attack. Initially, the attack is designed as an
untargeted attack. However, it can be modified to be a targeted attack.
The idea of the FGSM attack is to compute the adversarial perturbation
by taking the sign of the gradient of the loss function with respect to
the input. The FGSM attack is a fast attack since it only requires one
step to compute the adversarial perturbation. This makes it a popular
attack in the adversarial robustness literature. However, it has been
shown that the FGSM attack is not effective against the adversarial
defenses. This is because the FGSM attack is a weak attack and it can be
easily defended by the adversarial defenses such as adversarial
training.</p>
<p>If the attack is untargeted, the formula tries to maximize the loss
function with respect to the input and correct label. If the attack is
targeted, the formula tries to minimize the loss function with respect
to the input and target label since the purpose of the targeted attack
is to get closer to the target label. The untargeted FGSM attack is
given in the equation below, and the targeted FGSM attack is given in
the subsequent equation.</p>
<div class="math notranslate nohighlight">
\[\text{adv}_x = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))\]</div>
<div class="math notranslate nohighlight">
\[\text{adv}_x = x - \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{adv}_x\)</span> is the adversarial image.</p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> is the original input image.</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the original label for <strong>untargeted</strong> attacks, or the
target label for <strong>targeted</strong> attacks.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a multiplier to ensure the perturbations are
small.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> represents the model parameters.</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\theta, x, y)\)</span> is the loss function used by the model.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="c-w">
<h2>C&amp;W<a class="headerlink" href="#c-w" title="Link to this heading"></a></h2>
<p>The Carlini and Wagner (C&amp;W) attack <a class="footnote-reference brackets" href="#id38" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a>, introduced by Nicholas
Carlini and David Wagner in 2017, is a sophisticated method of
adversarial attack aimed at machine learning models, particularly those
used in computer vision. As an iterative, white-box attack, it requires
access to the model’s architecture and parameters. The core of the C&amp;W
attack involves formulating and solving an optimization problem that
minimally perturbs the input image in a way that leads to incorrect
model predictions. This is done while maintaining the perturbations
imperceptible to the human eye, thus preserving the image’s visual
integrity. The optimization process often employs techniques like binary
search to find the smallest possible perturbation that can deceive the
model. The C&amp;W attack is versatile, capable of being deployed as both
untargeted and targeted attacks. The attack can also use different
distance metrics when computing the perturbation, such as the L0, L2,
and L-infinity norms. The choice of distance metric can affect the
attack’s effectiveness and the perturbation’s perceptibility.</p>
<p>The attack’s effectiveness lies in its ability to subtly manipulate the
input data, challenging the robustness and security of machine learning
models, and it has become a benchmark for testing the vulnerability of
these models to adversarial examples. However, the biggest drawback of
the C&amp;W attack is its computational complexity, which stems from the
iterative nature of the attack and the optimization problem that needs
to be solved. This makes the C&amp;W attack less practical for real-world
applications.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp; \text{minimize} \quad \|\delta\|_p + c \cdot f(x + \delta) \\
&amp; \text{such that} \quad x + \delta \in [0,1]^n
\end{aligned}\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> is the perturbation added to the input image
<span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\|\delta\|_p\)</span> is the p-norm of the perturbation, which
measures the size of the perturbation.</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is a constant that balances the perturbation magnitude and
the success of the attack.</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x + \delta)\)</span> is the objective function, designed to mislead
the model into making incorrect predictions.</p></li>
<li><p><span class="math notranslate nohighlight">\(x + \delta \in [0,1]^n\)</span> ensures that the perturbed input
remains within the valid input range for the model.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="pgd">
<h2>PGD<a class="headerlink" href="#pgd" title="Link to this heading"></a></h2>
<p>The Projected Gradient Descent (PGD) attack <a class="footnote-reference brackets" href="#id39" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a> is a prominent
adversarial attack method in the field of machine learning, particularly
for evaluating the robustness of models against adversarial examples.
Introduced by Madry et al. <a class="footnote-reference brackets" href="#id40" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a>, the PGD attack is an iterative method
that generates adversarial examples by repeatedly applying a small
perturbation and projecting this perturbation onto an
<span class="math notranslate nohighlight">\(\varepsilon\)</span>-ball around the original input within a specified
norm. This process is repeated for a fixed number of steps or until a
successful adversarial example is found. The PGD attack operates under a
white-box setting, where the attacker has full knowledge of the model,
including its architecture and parameters. The strength of the PGD
attack lies in its simplicity and effectiveness in finding adversarial
examples within a constrained space, making it a standard benchmark in
adversarial robustness research. According to cite here, the PGD attack
is one of the most used attacks in adversarial training. However,
similar to other adversarial attacks like the C&amp;W attack, the PGD attack
can be computationally intensive, particularly when dealing with complex
models and high-dimensional input spaces, which may limit its
practicality in real-world scenarios.</p>
</section>
<hr class="docutils" />
<section id="deepfool">
<h2>DeepFool<a class="headerlink" href="#deepfool" title="Link to this heading"></a></h2>
<p>The DeepFool attack, introduced by Moosavi-Dezfooli et al. <a class="footnote-reference brackets" href="#id41" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a> in
2016, is a type of adversarial attack that aims to generate adversarial
examples that are close to the original input but mislead the model. It
is an iterative, white-box attack. The algorithm works by linearizing
the decision boundaries of the model and then applying a small
perturbation that pushes the input just across this boundary. This
process is repeated iteratively until the input is misclassified,
ensuring that the resulting adversarial example is as close to the
original input as possible. One of the key strengths of DeepFool is its
ability to compute these minimal perturbations with relatively low
computational overhead compared to other methods because of its
linearization approach. Despite its efficiency, the attack assumes a
somewhat idealized linear model, which may not always accurately reflect
the complex decision boundaries in more advanced, non-linear models.
Nonetheless, DeepFool has become a valuable tool in the adversarial
machine learning toolkit for its ability to provide insights into model
vulnerabilities with minimal perturbations.</p>
</section>
<hr class="docutils" />
<section id="decision-boundary">
<h2>Decision Boundary<a class="headerlink" href="#decision-boundary" title="Link to this heading"></a></h2>
<p>The Decision Boundary attack is a black-box attack that was introduced
by Brendel et al. <a class="footnote-reference brackets" href="#id42" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a> in 2017. The idea of the Decision Boundary
attack is to find the decision boundary of the model and then apply a
small perturbation that pushes the input just across this boundary. The
Decision Boundary attack is an iterative attack and can be both targeted
and untargeted. The attack starts with a random input that is initially
adversarial and then iteratively updates the input to get closer to the
decision boundary and minimize the perturbation. The advantage of the
attack is that it does not require any information about the model. This
makes it more suitable for real-world applications where the model’s
information is not available. However, the drawback of the attack is
that it is computationally expensive since it requires iteratively
updating the input to get closer to the decision boundary.</p>
</section>
<hr class="docutils" />
<section id="lots">
<h2>LOTS<a class="headerlink" href="#lots" title="Link to this heading"></a></h2>
<p>LOTS, Layerwise Origin-Target Synthesis <a class="footnote-reference brackets" href="#id43" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a>, is a type of adversarial
attack that was introduced by Rozsa et al. in 2017. It is a versatile,
white-box attack that can be used as both targeted and untargeted
attacks, single-step and iterative. The idea of the LOTS attack is to
compute the adversarial perturbation by using the deep feature layers of
the model. The purpose of the attack algorithm is to adjust the deep
feature representation of the input to match the deep feature
representation of the target class. Utilizing deep feature
representations makes the LOTS attack suitable for systems that use deep
feature representations, such as face recognition systems. The results
show that the Iterative LOTS attack is highly successful against the VGG
Face network with success rates between 98.28% to 100% <a class="footnote-reference brackets" href="#id44" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></a>. However,
the drawback of the LOTS attack is that it needs to know the deep
feature representation of the target class.</p>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id23" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id24" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., &amp;
Mukhopadhyay, D. (2018). Adversarial Attacks and Defences: A Survey.
arXiv preprint arXiv:1810.00069.</p>
</aside>
<aside class="footnote brackets" id="id25" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id26" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<p>Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., &amp;
Mukhopadhyay, D. (2018). Adversarial Attacks and Defences: A Survey.
arXiv preprint arXiv:1810.00069.</p>
</aside>
<aside class="footnote brackets" id="id27" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id28" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<p>Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., &amp;
Mukhopadhyay, D. (2018). Adversarial Attacks and Defences: A Survey.
arXiv preprint arXiv:1810.00069.</p>
</aside>
<aside class="footnote brackets" id="id29" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<p>Papernot, N., Mcdaniel, P., &amp; Goodfellow, I. J. (2016).
Transferability in Machine Learning: from Phenomena to Black-Box
Attacks using Adversarial Samples. arXiv preprint arXiv:1605.07277.
Retrieved from https://api.semanticscholar.org/CorpusID:17362994</p>
</aside>
<aside class="footnote brackets" id="id30" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<p>Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., &amp;
Swami, A. (2017). Practical Black-Box Attacks against Machine
Learning. arXiv preprint arXiv:1602.02697.</p>
</aside>
<aside class="footnote brackets" id="id31" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">9</a><span class="fn-bracket">]</span></span>
<p>Brendel, W., Rauber, J., &amp; Bethge, M. (2017). Decision-Based
Adversarial Attacks: Reliable Attacks Against Black-Box Machine
Learning Models. arXiv preprint arXiv:1712.04248. Retrieved from
https://api.semanticscholar.org/CorpusID:2410333</p>
</aside>
<aside class="footnote brackets" id="id32" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">10</a><span class="fn-bracket">]</span></span>
<p>Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., &amp; Hsieh, C.-J. (2017).
ZOO: Zeroth Order Optimization Based Black-Box Attacks to Deep Neural
Networks without Training Substitute Models. In <em>Proceedings of the
10th ACM Workshop on Artificial Intelligence and Security</em>
(pp. 15-26). New York, NY, USA: Association for Computing Machinery.
doi:
<a class="reference external" href="https://doi.org/10.1145/3128572.3140448">10.1145/3128572.3140448</a></p>
</aside>
<aside class="footnote brackets" id="id33" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">11</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id34" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">12</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id35" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">13</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id36" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">14</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id37" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">15</a><span class="fn-bracket">]</span></span>
<p>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. (2015). Explaining and
Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572.</p>
</aside>
<aside class="footnote brackets" id="id38" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">16</a><span class="fn-bracket">]</span></span>
<p>Carlini, N., &amp; Wagner, D. (2017). “Towards Evaluating the Robustness
of Neural Networks.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1702.04267">arXiv:1702.04267</a></p>
</aside>
<aside class="footnote brackets" id="id39" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">17</a><span class="fn-bracket">]</span></span>
<p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2017).
“Towards Deep Learning Models Resistant to Adversarial Attacks.”
Available at: <a class="reference external" href="https://arxiv.org/abs/1706.06083">arXiv:1706.06083</a></p>
</aside>
<aside class="footnote brackets" id="id40" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">18</a><span class="fn-bracket">]</span></span>
<p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2017).
“Towards Deep Learning Models Resistant to Adversarial Attacks.”
Available at: <a class="reference external" href="https://arxiv.org/abs/1706.06083">arXiv:1706.06083</a></p>
</aside>
<aside class="footnote brackets" id="id41" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">19</a><span class="fn-bracket">]</span></span>
<p>Moosavi-Dezfooli, S.-M., Fawzi, A., &amp; Frossard, P. (2016). “DeepFool:
A Simple and Accurate Method to Fool Deep Neural Networks.” Available
at: <a class="reference external" href="https://arxiv.org/abs/1511.04599">arXiv:1511.04599</a></p>
</aside>
<aside class="footnote brackets" id="id42" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">20</a><span class="fn-bracket">]</span></span>
<p>Brendel, W., Rauber, J., &amp; Bethge, M. (2017). “Decision-Based
Adversarial Attacks: Reliable Attacks Against Black-Box Machine
Learning Models.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1712.04248">arXiv:1712.04248</a></p>
</aside>
<aside class="footnote brackets" id="id43" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">21</a><span class="fn-bracket">]</span></span>
<p>Rozsa, A., Rudd, E. M., &amp; Boult, T. E. (2017). “LOTS: Layerwise
Origin-Target Synthesis for Adversarial Attack.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1611.06503">arXiv:1611.06503</a></p>
</aside>
<aside class="footnote brackets" id="id44" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">22</a><span class="fn-bracket">]</span></span>
<p>Rozsa, A., Rudd, E. M., &amp; Boult, T. E. (2017). “LOTS: Layerwise
Origin-Target Synthesis for Adversarial Attack.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1611.06503">arXiv:1611.06503</a></p>
</aside>
</aside>
</section>
</section>
<section id="adversarial-defenses">
<h1>Adversarial Defenses<a class="headerlink" href="#adversarial-defenses" title="Link to this heading"></a></h1>
<p>Adversarial training is a methodology in machine learning, particularly within the field of deep learning, aimed at improving the robustness and generalization of models against adversarial examples. These are inputs deliberately crafted to deceive models into making incorrect predictions or classifications <a href="#id76"><span class="problematic" id="id45">[1]_</span></a>. The concept of adversarial training emerged as a critical response to the observation that neural networks, despite their high accuracy, are often vulnerable to subtly modified inputs that are imperceptible to humans <a href="#id77"><span class="problematic" id="id46">[2]_</span></a>.</p>
<p>The core idea behind adversarial training involves the intentional generation of adversarial examples during the training process. By exposing the model to these challenging scenarios, the model learns to generalize better and becomes more resistant to such attacks <a href="#id78"><span class="problematic" id="id47">[3]_</span></a>. So far, adversarial training represents the only known defense that works to some extent and scale against adversarial attacks <a href="#id79"><span class="problematic" id="id48">[4]_</span></a>.</p>
<p><img alt="adversarial_training" src="https://github.com/melihcatal/advsecurenet/assets/46859098/da22a465-03b5-4583-97ac-fded10999073" /> <em>Adversarial Training Flow</em>
<img alt="adversarial_data_generator" src="https://github.com/melihcatal/advsecurenet/assets/46859098/4ac0a8cb-a97f-4363-b05e-06444f7cd0f5" /> <em>Adversarial Data Generator</em></p>
<p>Ensemble Adversarial Training, proposed by Florian Tramèr et al. <a href="#id80"><span class="problematic" id="id49">[5]_</span></a> in 2018, is a type of adversarial training that aims to improve the robustness of the model to unseen attacks and black-box attacks by generalizing the adversarial training process. The idea of the ensemble adversarial training is crafting adversarial examples from a set of pretrained substitute models in addition to the adversarial examples crafted from the original source model that the defender wants to robustify. The intuition is that crafting adversarial samples only from the source model can lead to overfitting to the source model and the model still can be vulnerable to unseen attacks and black-box attacks. However, crafting adversarial samples from a set of pretrained substitute models can lead to generalization of the adversarial training process and improve the robustness of the model to unseen attacks and black-box attacks. The experiments <a href="#id81"><span class="problematic" id="id50">[5]_</span></a> showed that the ensemble adversarial training can improve the robustness of the model to unseen attacks and black-box attacks but lower the accuracy on clean examples.</p>
<p>The ensemble feature of the ensemble adversarial training refers to ensemble of models. However, it is also possible to ensemble the adversarial attacks <a href="#id82"><span class="problematic" id="id51">[5]_</span></a>. The intuition is similar to the ensemble of models, which is generalization since the adversarial training does not offer a guarantee to the unseen attacks <a href="#id83"><span class="problematic" id="id52">[6]_</span></a> <a href="#id84"><span class="problematic" id="id53">[7]_</span></a>. It’s also shown that having a robust model to one type of attack can make the model more vulnerable to other types of attacks <a href="#id85"><span class="problematic" id="id54">[7]_</span></a> <a href="#id86"><span class="problematic" id="id55">[8]_</span></a>.</p>
<p>The ensemble of adversarial attacks refers to crafting adversarial examples from a set of adversarial attacks in addition to the adversarial examples crafted from the original adversarial attack. The purpose is having a robust model to different types of perturbations simultaneously <a href="#id87"><span class="problematic" id="id56">[5]_</span></a>. However, the results show that the models trained with ensemble of adversarial attacks are not robust as the models trained with each attack individually <a href="#id88"><span class="problematic" id="id57">[5]_</span></a>.</p>
<p><img alt="ensemble_adversarial_generator" src="https://github.com/melihcatal/advsecurenet/assets/46859098/23601123-2566-468b-b0c5-b1eaa0bf8069" />  <em>Ensemble Adversarial Generator. The generator randomly picks one model from the models pool and one attack from the attacks pool. Only having origin model and one attack is the same as classical adversarial training. Having one attack but multiple pretrained models is the Ensemble Adversarial Training. It’s also possible have ensemble models and ensemble attacks at the same time.</em></p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id58" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. (2015). Explaining and Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572.</p>
</aside>
<aside class="footnote brackets" id="id59" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., &amp; Fergus, R. (2013). Intriguing properties of neural networks. CoRR, abs/1312.6199. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:604334">https://api.semanticscholar.org/CorpusID:604334</a></p>
</aside>
<aside class="footnote brackets" id="id60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2017). Towards Deep Learning Models Resistant to Adversarial Attacks. ArXiv, abs/1706.06083. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:3488815">https://api.semanticscholar.org/CorpusID:3488815</a></p>
</aside>
<aside class="footnote brackets" id="id61" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<p>Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., Madry, A., &amp; Kurakin, A. (2019). On Evaluating Adversarial Robustness. arXiv preprint arXiv:1902.06705.</p>
</aside>
<aside class="footnote brackets" id="id62" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<p>Tramer, F., Kurakin, A., Papernot, N., Boneh, D., &amp; Mcdaniel, P. (2017). Ensemble Adversarial Training: Attacks and Defenses. ArXiv, abs/1705.07204. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:21946795">https://api.semanticscholar.org/CorpusID:21946795</a></p>
</aside>
<aside class="footnote brackets" id="id63" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<p>Papernot, N., Mcdaniel, P., &amp; Goodfellow, I. J. (2016). Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. ArXiv, abs/1605.07277. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:17362994">https://api.semanticscholar.org/CorpusID:17362994</a></p>
</aside>
<aside class="footnote brackets" id="id64" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<p>Schott, L., Rauber, J., Bethge, M., &amp; Brendel, W. (2018). Towards the first adversarially robust neural network model on MNIST. arXiv preprint arXiv:1805.09190.</p>
</aside>
<aside class="footnote brackets" id="id65" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<p>Engstrom, L., Tsipras, D., Schmidt, L., &amp; Madry, A. (2017). A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. ArXiv, abs/1712.02779. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:21929206">https://api.semanticscholar.org/CorpusID:21929206</a></p>
</aside>
</aside>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">AdvSecureNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">How to Contribute?</a></li>
<li class="toctree-l1"><a class="reference internal" href="attacks.html">Adversarial Attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="defenses.html">Adversarial Defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluations.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
</ul>
</div>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction.html" class="btn btn-neutral float-right" title="AdvSecureNet" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Melih Catal.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>