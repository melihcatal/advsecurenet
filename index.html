<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AdvSecureNet Documentation &mdash; AdvSecureNet  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=5929fcd5"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="AdvSecureNet" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            AdvSecureNet
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">AdvSecureNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="attacks.html">Adversarial Attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="defenses.html">Adversarial Defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluations.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">AdvSecureNet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">AdvSecureNet Documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="advsecurenet-documentation">
<h1>AdvSecureNet Documentation<a class="headerlink" href="#advsecurenet-documentation" title="Link to this heading"></a></h1>
</section>
<section id="advsecurenet">
<h1>AdvSecureNet<a class="headerlink" href="#advsecurenet" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Quality Gate Status" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=alert_status" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Bugs" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=bugs" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Code Smells" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=code_smells" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Coverage" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=coverage" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Duplicated Lines (%)" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=duplicated_lines_density" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Lines of Code" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=ncloc" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Reliability Rating" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=reliability_rating" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Security Rating" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=security_rating" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Technical Debt" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=sqale_index" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Maintainability Rating" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=sqale_rating" /></a> <a class="reference external" href="https://sonarcloud.io/summary/new_code?id=melihcatal_advsecurenet"><img alt="Vulnerabilities" src="https://sonarcloud.io/api/project_badges/measure?project=melihcatal_advsecurenet&amp;metric=vulnerabilities" /></a> <a class="reference external" href="https://github.com/melihcatal/advsecurenet/actions/workflows/python-ci.yml"><img alt="Unit Tests and Style Checks" src="https://github.com/melihcatal/advsecurenet/actions/workflows/python-ci.yml/badge.svg?branch=develop" /></a> <a class="reference external" href="https://github.com/melihcatal/advsecurenet/actions/workflows/documentation.yml"><img alt="Build and Deploy Sphinx Documentation" src="https://github.com/melihcatal/advsecurenet/actions/workflows/documentation.yml/badge.svg" /></a> <a class="reference external" href="https://github.com/melihcatal/advsecurenet/actions/workflows/python-publish.yml"><img alt="Upload Python Package" src="https://github.com/melihcatal/advsecurenet/actions/workflows/python-publish.yml/badge.svg" /></a></p>
<p align="center"></p><p><img alt="advsecurenet_logo" src="https://github.com/melihcatal/advsecurenet/assets/46859098/cdad6b95-5a40-491f-a3d1-c85a3976d681" />
AdvSecureNet is a Python library for Machine Learning Security, developed by <a class="reference external" href="https://github.com/melihcatal">Melih Catal</a> at <a class="reference external" href="https://www.uzh.ch/en.html">University of Zurich</a> as part of his Master’s Thesis under the supervision of <a class="reference external" href="https://www.ifi.uzh.ch/en/aiml/people/guenther.html">Prof. Dr. Manuel Günther</a>. The main focus of the library is on adversarial attacks and defenses for vision tasks, with plans to extend support to other tasks such as natural language processing.</p>
<p>The library provides tools to generate adversarial examples, evaluate the robustness of machine learning models against adversarial attacks, and train robust machine learning models.
Built on top of <a class="reference external" href="https://pytorch.org/">PyTorch</a>, it is designed to be modular and extensible, making it easy to run experiments with different configurations.
AdvSecureNet supports multi-GPU setups to enhance computational efficiency and fully supports both CLI and API interfaces, along with external YAML configuration files, enabling comprehensive testing and evaluation, facilitating the sharing and reproducibility of experiments.</p>
<section id="features">
<h2>Features<a class="headerlink" href="#features" title="Link to this heading"></a></h2>
<p><strong>Adversarial Attacks:</strong> AdvSecureNet supports a diverse range of
evasion attacks on computer vision tasks, including gradient-based,
decision-based, single-step, iterative, white-box, black-box, targeted,
and untargeted attacks, enabling comprehensive testing and evaluation of
neural network robustness against various types of adversarial examples.</p>
<p><strong>Adversarial Defenses:</strong> The toolkit includes adversarial training and
ensemble adversarial training. Adversarial training incorporates
adversarial examples into the training process to improve model
robustness, while ensemble adversarial training uses multiple models or
attacks for a more resilient defense strategy.</p>
<p><strong>Evaluation Metrics:</strong> AdvSecureNet supports metrics like accuracy,
robustness, transferability, and similarity. Accuracy measures
performance on clean data, robustness assesses resistance to attacks,
transferability evaluates how well adversarial examples deceive
different models, and similarity quantifies perceptual differences using
PSNR and SSIM.</p>
<p><strong>Multi-GPU Support:</strong> AdvSecureNet is optimized for multi-GPU setups,
enhancing the efficiency of training, evaluation, and adversarial attack
generation, especially for large models and datasets or complex methods.
By utilizing multiple GPUs in parallel, AdvSecureNet aims to reduce
computational time, making it ideal for large-scale experiments and deep
learning models.</p>
<p><strong>CLI and API Interfaces:</strong> AdvSecureNet offers both CLI and API
interfaces. The CLI allows for quick execution of attacks, defenses, and
evaluations, while the API provides advanced integration and extension
within user applications.</p>
<p><strong>External Configuration Files:</strong> The toolkit supports YAML
configuration files for easy parameter tuning and experimentation. This
feature enables users to share experiments, reproduce results, and
manage setups effectively, facilitating collaboration and comparison.</p>
<p><strong>Built-in Models and Datasets Support:</strong> AdvSecureNet supports all
PyTorch vision library models and well-known datasets like CIFAR-10,
CIFAR-100, MNIST, FashionMNIST, and SVHN. Users can start without
additional setup, but the toolkit also allows for custom datasets and
models, offering flexibility for various research and applications.</p>
</section>
<section id="supported-attacks">
<h2><a class="reference internal" href="attacks.html"><span class="doc">Supported Attacks</span></a><a class="headerlink" href="#supported-attacks" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1412.6572">FGSM - FGSM Targeted</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1706.06083">PGD - PGD Targeted</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1511.04599">DeepFool</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1608.04644">CW</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1611.06179">LOTS</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1712.04248">Decision Boundary</a></p></li>
</ul>
</section>
<section id="supported-defenses">
<h2><a class="reference internal" href="defenses.html"><span class="doc">Supported Defenses</span></a><a class="headerlink" href="#supported-defenses" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1412.6572">Adversarial Training</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1705.07204">Ensemble Adversarial Training</a></p></li>
</ul>
</section>
<section id="supported-evaluation-metrics">
<h2><a class="reference internal" href="evaluations.html"><span class="doc">Supported Evaluation Metrics</span></a><a class="headerlink" href="#supported-evaluation-metrics" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Benign Accuracy</p></li>
<li><p>Attack Success Rate</p></li>
<li><p>Transferability</p></li>
<li><p>Perturbation Distance</p></li>
<li><p>Robustness Gap</p></li>
<li><p>Perturbation Effectiveness</p></li>
</ul>
<section id="similarity-metrics">
<h3>Similarity Metrics<a class="headerlink" href="#similarity-metrics" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">PSNR - Peak Signal-to-Noise
Ratio</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Structural_similarity">SSIM - Structural Similarity
Index</a></p></li>
</ul>
</section>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h2>
<p>You can install the library using <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>advsecurenet
</pre></div>
</div>
<p>Or install it from source:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/melihcatal/advsecurenet.git
<span class="nb">cd</span><span class="w"> </span>advsecurenet
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
</section>
<section id="why-advsecurenet">
<h2>Why AdvSecureNet?<a class="headerlink" href="#why-advsecurenet" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Research-Oriented</strong>: Easily run and share experiments with
different configurations using YAML configuration files.</p></li>
<li><p><strong>Supports Various Attacks and Defenses</strong>: Experiment with a wide
range of adversarial attacks and defenses.</p></li>
<li><p><strong>Supports Any PyTorch Model</strong>: Use pre-trained models or your own
PyTorch models with the library.</p></li>
<li><p><strong>Supports Various Evaluation Metrics</strong>: Evaluate the robustness of
models, performance of adversarial attacks, and defenses.</p></li>
<li><p><strong>Bening Use Case Support</strong>: Train and evaluate models on benign
data.</p></li>
<li><p><strong>Native Multi-GPU Support</strong>: Efficiently run large-scale experiments
utilizing multiple GPUs.</p></li>
</ul>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h2>
<p>The library can be used as a command line tool or as an importable
Python package.</p>
<section id="command-line-tool">
<h3>Command Line Tool<a class="headerlink" href="#command-line-tool" title="Link to this heading"></a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">advsecurenet</span></code> command to interact with the library. Use
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span> <span class="pre">--help</span></code> to see available commands and options. It is
recommended to use YAML configuration files to run experiments. You can
list the available configuration options using
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span> <span class="pre">utils</span> <span class="pre">configs</span> <span class="pre">list</span></code> and generate a template
configuration file using
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span> <span class="pre">utils</span> <span class="pre">configs</span> <span class="pre">get</span> <span class="pre">-c</span> <span class="pre">&lt;config_name&gt;</span> <span class="pre">-o</span> <span class="pre">&lt;output_file&gt;</span></code>.</p>
<p>Running an adversarial attack:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>advsecurenet<span class="w"> </span>attack<span class="w"> </span>-c<span class="w"> </span>./fgsm.yml
</pre></div>
</div>
<p>Running an adversarial defense:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>advsecurenet<span class="w"> </span>defense<span class="w"> </span>adversarial-training<span class="w"> </span>-c<span class="w"> </span>./adv_training.yml
</pre></div>
</div>
<p>Running an evaluation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>advsecurenet<span class="w"> </span>evaluate<span class="w"> </span>benign<span class="w"> </span>-c<span class="w"> </span>./evaluate_benign.yml

or

advsecurenet<span class="w"> </span>evaluate<span class="w"> </span>adversarial<span class="w"> </span>-c<span class="w"> </span>./evaluate_adversarial.yml
</pre></div>
</div>
</section>
<section id="python-package">
<h3>Python Package<a class="headerlink" href="#python-package" title="Link to this heading"></a></h3>
<p>You can import the library as a Python package. You can use the
<code class="docutils literal notranslate"><span class="pre">advsecurenet</span></code> module to access the library. You can find the
available modules and classes in the
<a class="reference external" href="http://melihcatal.github.io/advsecurenet/">documentation</a>.</p>
<p><img alt="image" src="https://github.com/melihcatal/advsecurenet/assets/46859098/f3f86817-8ac3-4523-8f5e-cc9d4b4cbcf3" /> <em>Usage example of AdvSecureNet demonstrating the equivalence
between a YAML configuration file with a command-line interface (CLI)
command and a corresponding Python API implementation.</em></p>
</section>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h2>
<p>Examples of different use cases can be found in the
<a class="reference external" href="https://github.com/melihcatal/advsecurenet/tree/main/examples/">examples</a> directory.</p>
</section>
<section id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Link to this heading"></a></h2>
<p>The high-level architecture of the toolkit is shown in the figure below.</p>
<figure class="align-default" id="id68">
<img alt="AdvSecureNet API Architecture" src="https://drive.switch.ch/index.php/s/SdKAyOZs1d9bcin/download" />
<figcaption>
<p><span class="caption-text">AdvSecureNet API Architecture</span><a class="headerlink" href="#id68" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-default">
<img alt="AdvSecureNet CLI Architecture" src="https://drive.switch.ch/index.php/s/ZbjIHBHql0dV6n0/download" />
</figure>
<p>The toolkit is designed to be modular and extensible. CLI and Python API
are implemented separately, however, they share the same core components
and they have the same package structure for the sake of consistency.
Tests are implemented for both CLI and Python API to ensure the
correctness of the implementation and again they follow the same
structure. The toolkit is designed to be easily extensible, new attacks,
defenses, and evaluation metrics can be added by implementing the
corresponding classes and registering them in the corresponding
registries.</p>
</section>
<section id="comparison-with-other-libraries">
<h2>Comparison with Other Libraries<a class="headerlink" href="#comparison-with-other-libraries" title="Link to this heading"></a></h2>
<p>AdvSecureNet stands out among adversarial machine learning toolkits like IBM ART, AdverTorch, SecML, FoolBox, ARES, and CleverHans. Key advantages include:</p>
<ul class="simple">
<li><p><strong>Active Maintenance:</strong> Ensures ongoing support and updates.</p></li>
<li><p><strong>Comprehensive Training Support:</strong> One of the few toolkits supporting both adversarial and ensemble adversarial training.</p></li>
<li><p><strong>Multi-GPU Support:</strong> The first toolkit with native multi-GPU support for attacks, defenses, and evaluations, ideal for large-scale experiments.</p></li>
<li><p><strong>Flexible Interfaces:</strong> The first toolkit that fully supports CLI, API usage, and external YAML configuration files for reproducibility for all features.</p></li>
<li><p><strong>Performance:</strong> AdvSecureNet excels in performance, significantly reducing execution times on multi-GPU setups. For example, the multi-GPU PGD attack time (107 seconds) is faster than ARES’s best single GPU time (183 seconds). Adversarial training time is reduced from 304 seconds on a single GPU to 166 seconds with 7 GPUs, a speedup of 1.83x.</p></li>
</ul>
<p><img alt="performance" src="https://github.com/melihcatal/advsecurenet/assets/46859098/33a4678c-4e22-4dc8-9929-d7c5c2e3c03b" /> <em>Performance Benchmark for AdvSecureNet and Other Toolkits</em>
<img alt="comparision" src="https://github.com/melihcatal/advsecurenet/assets/46859098/48744f4c-afae-48ff-8c39-2dea55ba8a3a" /> <em>[1] SecML supports attacks from CleverHans and FoolBox [2] This feature is only available for adversarial training.</em></p>
</section>
<section id="license">
<h2>License<a class="headerlink" href="#license" title="Link to this heading"></a></h2>
<p>This project is licensed under the terms of the MIT license. See
<a class="reference external" href="https://github.com/melihcatal/advsecurenet/blob/main/LICENSE">LICENSE</a> for more details.</p>
</section>
</section>
<section id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading"></a></h1>
<section id="attack-success-rate">
<h2>Attack Success Rate<a class="headerlink" href="#attack-success-rate" title="Link to this heading"></a></h2>
<p>Attack Success Rate (ASR) is the fundamental metric for evaluating the adversarial robustness of the model. It is the percentage of the adversarial examples that are successfully misclassified by the model. In the context of adversarial attacks, success can be defined differently for targeted and untargeted attacks. For targeted attacks, success means that the adversarial example is classified as a specific target class. For untargeted attacks, success means that the adversarial example is classified as any class other than the true class. The higher the ASR, the more vulnerable the model is to adversarial attacks. The ASR is given by the equation below.</p>
<div class="math notranslate nohighlight">
\[ASR = \frac{\text{Number of Successful Adversarial Examples}}{\text{Total Number of Adversarial Examples}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><strong>Number of Successful Adversarial Examples</strong> refers to the count of adversarial examples that lead to a successful attack, where success is defined as:
- In targeted attacks, the adversarial example is classified as the target class, <em>y:sub:`target`</em>.
- In untargeted attacks, the adversarial example is classified as any class other than the true class, <em>y:sub:`true`</em>.</p></li>
<li><p><strong>Total Number of Adversarial Examples</strong> is the total number of adversarial examples generated from inputs where the model initially makes a correct prediction.</p></li>
</ul>
</section>
<section id="adversarial-transferability-evaluation">
<h2>Adversarial Transferability Evaluation<a class="headerlink" href="#adversarial-transferability-evaluation" title="Link to this heading"></a></h2>
<p>The evaluation of adversarial transferability is facilitated by the <code class="docutils literal notranslate"><span class="pre">transferability_evaluator</span></code> class within the <code class="docutils literal notranslate"><span class="pre">advsecurenet.evaluation.evaluators</span></code> module. This evaluator is designed to assess the effectiveness of adversarial examples, originally generated for a source model, in deceiving various target models.</p>
<p>The evaluator is initialized with a list of target models. During the evaluation phase, the <code class="docutils literal notranslate"><span class="pre">update</span></code> method calculates whether the adversarial examples successfully mislead both the source and the target models. Success in targeted attacks is determined by the adversarial example being classified as the target class, while in untargeted attacks, success is achieved if the example is classified as any class other than its true class.</p>
<p>The evaluator maintains a tally of successful deceptions for each target model, as well as the total count of adversarial examples that successfully deceive the source model. The transferability rate for each target model is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[Transferability\ Rate = \frac{\text{Number of Successful Transfers to Target Model}}{\text{Total Number of Successful Adversarial Examples on Source Model}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><strong>Number of Successful Transfers to Target Model</strong> is the count of adversarial examples that successfully deceive the target model.</p></li>
<li><p><strong>Total Number of Successful Adversarial Examples on Source Model</strong> refers to the count of adversarial examples that initially misled the source model.</p></li>
</ul>
<p>This evaluation method provides a thorough analysis of the transferability of adversarial examples across different models, shedding light on the robustness of each model against such attacks.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Consider a scenario with a source model, Model_A, and two target models, Model_B and Model_C. Suppose Model_A generates 100 adversarial examples, out of which 80 successfully deceive Model_A. When these 80 adversarial examples are tested against Model_B, 50 are successful, and against Model_C, 30 are successful.</p>
<p>Using the transferability rate formula:</p>
<p>For Model_B:</p>
<div class="math notranslate nohighlight">
\[Transferability\ Rate_{Model_B} = \frac{50}{80} = 0.625\]</div>
<p>For Model_C:</p>
<div class="math notranslate nohighlight">
\[Transferability\ Rate_{Model_C} = \frac{30}{80} = 0.375\]</div>
<p>This indicates that adversarial examples from Model_A are more transferable to Model_B than to Model_C.</p>
</div>
</section>
<section id="robustness-gap">
<h2>Robustness Gap<a class="headerlink" href="#robustness-gap" title="Link to this heading"></a></h2>
<p>Robustness Gap is a metric that measures the difference between the accuracy on clean examples and the accuracy on adversarial examples. The higher the robustness gap is, the more vulnerable the model is to adversarial attacks. Possible values for the robustness gap are between 0 and 1. 0 means that the model performs the same on clean and adversarial examples. 1 means that the model performs perfectly on clean examples but completely fails on adversarial examples.</p>
<p><strong>Clean Accuracy</strong> (A<sub>clean</sub>) is calculated as the ratio of the total number of correctly classified clean images (N<sub>correct_clean</sub>) to the total number of samples (N<sub>total</sub>):</p>
<div class="math notranslate nohighlight">
\[A_{\text{clean}} = \frac{N_{\text{correct_clean}}}{N_{\text{total}}}\]</div>
<p><strong>Adversarial Accuracy</strong> (A<sub>adv</sub>) is the ratio of the total number of correctly classified adversarial images (N<sub>correct_adv</sub>) to the total number of samples (N<sub>total</sub>):</p>
<div class="math notranslate nohighlight">
\[A_{\text{adv}} = \frac{N_{\text{correct_adv}}}{N_{\text{total}}}\]</div>
<p><strong>The Robustness Gap</strong> (G<sub>robust</sub>) is the difference between Clean Accuracy and Adversarial Accuracy:</p>
<div class="math notranslate nohighlight">
\[G_{\text{robust}} = A_{\text{clean}} - A_{\text{adv}}\]</div>
<p>Evaluator for the perturbation effectiveness. The effectiveness score is the attack success rate divided by the perturbation distance. The higher the score, the more effective the attack.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Suppose we have a model tested on a dataset of 1000 images. Out of these, the model correctly classifies 950 clean images, giving us N<sub>correct_clean</sub> = 950 and N<sub>total</sub> = 1000. When exposed to adversarial examples, the model correctly classifies only 700 of these images, thus N<sub>correct_adv</sub> = 700.</p>
<p>Using the provided formulas, we can calculate the Clean Accuracy (A<sub>clean</sub>) and the Adversarial Accuracy (A<sub>adv</sub>):</p>
<div class="math notranslate nohighlight">
\[A_{\text{clean}} = \frac{950}{1000} = 0.95\]</div>
<div class="math notranslate nohighlight">
\[A_{\text{adv}} = \frac{700}{1000} = 0.70\]</div>
<p>Then, the Robustness Gap (G<sub>robust</sub>) is calculated as:</p>
<div class="math notranslate nohighlight">
\[G_{\text{robust}} = A_{\text{clean}} - A_{\text{adv}} = 0.95 - 0.70 = 0.25\]</div>
<p>This Robustness Gap of 0.25 indicates that the model’s performance significantly degrades when exposed to adversarial examples, revealing a vulnerability to such attacks.</p>
</div>
</section>
<section id="perturbation-effectiveness">
<h2>Perturbation Effectiveness<a class="headerlink" href="#perturbation-effectiveness" title="Link to this heading"></a></h2>
<p>Perturbation Effectiveness is a metric for evaluating the effectiveness of the adversarial perturbation. It is the percentage of the adversarial perturbation that is effective in changing the model’s prediction. The higher the perturbation effectiveness is, the more effective the adversarial perturbation is. The purpose of this metric is to distinguish between attacks that have a high success rate but require a large perturbation magnitude, and attacks that have a lower success rate but require a smaller perturbation magnitude. The perturbation effectiveness is given by the equation below.</p>
<div class="math notranslate nohighlight">
\[PE = \frac{\text{Attack Success Rate}}{\text{Perturbation}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><strong>Attack Success Rate</strong> is the percentage of the adversarial examples that are successfully misclassified by the model.</p></li>
<li><p><strong>Perturbation</strong> is the perturbation magnitude of the adversarial examples. It can be measured using different norms, such as L1, L2, or Linf.</p></li>
</ul>
</section>
<section id="id1">
<h2>Similarity Metrics<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<p>The Peak Signal-to-Noise Ratio (PSNR) metric is a standard used in the field of image processing for assessing the quality of reconstructed or compressed images in relation to the original ones. The PSNR is derived from the mean squared error (MSE) between the original image and the reconstructed one. It is typically expressed in decibels (dB), indicating the ratio of the maximum possible power of a signal to the power of corrupting noise.</p>
<p>The formula for PSNR is given by:</p>
<div class="math notranslate nohighlight">
\[\text{PSNR} = 10 \cdot \log_{10} \left( \frac{\text{MAX}_I^2}{\text{MSE}} \right)\]</div>
<p>where MAX_I represents the maximum possible pixel value of the image (e.g., 255 for 8-bit images), and MSE is the mean squared error between the original and reconstructed images.</p>
<p>The range of PSNR is typically between 0 dB to infinity, with higher values indicating a smaller difference between the original and reconstructed image, and thus, better quality. In cases where the original and reconstructed images are identical, the MSE becomes zero, leading to an undefined PSNR in the logarithmic scale, which can be theoretically considered as infinite. A higher PSNR value generally suggests that the reconstructed image closely resembles the original image in quality.</p>
<p>The Structural Similarity Index Measure (SSIM) is a metric used for measuring the similarity between two images. Unlike traditional methods like PSNR that focus on pixel-level differences, SSIM considers changes in structural information, luminance, and contrast, providing a more perceptually relevant assessment of image quality.</p>
<p>The formula for SSIM is given by:</p>
<div class="math notranslate nohighlight">
\[\text{SSIM}(x, y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are the two images being compared, <span class="math notranslate nohighlight">\(\mu_x\)</span>, <span class="math notranslate nohighlight">\(\mu_y\)</span> are the average pixel values, <span class="math notranslate nohighlight">\(\sigma_x^2\)</span>, <span class="math notranslate nohighlight">\(\sigma_y^2\)</span> are the variances, <span class="math notranslate nohighlight">\(\sigma_{xy}\)</span> is the covariance of the images, and <span class="math notranslate nohighlight">\(C_1\)</span>, <span class="math notranslate nohighlight">\(C_2\)</span> are constants used to stabilize the division.</p>
<p>The SSIM index is a decimal value between -1 and 1, where a value of 1 for SSIM implies no difference between the compared images. As the value decreases, the differences between the images increase. SSIM is particularly useful in contexts where a human observer’s assessment of quality is important, as it aligns more closely with human visual perception than metrics based solely on pixel differences, which makes it suitable for adversarial robustness evaluation.</p>
<p><img alt="ssim_psnr" src="https://github.com/melihcatal/advsecurenet/assets/46859098/ac7344d0-cef0-4ec3-b784-9aec32a0c80d" /> <em>Comparison of SSIM and PSNR Metrics vs. Epsilon in FGSM Attack</em></p>
<p><img alt="ssim_psnr_example" src="https://github.com/melihcatal/advsecurenet/assets/46859098/b8e3d753-5bf5-4963-9f1b-b36df3acae5d" /> <em>SSIM and PSNR Example. Taken from https://medium.com/&#64;datamonsters/a-quick-overview-of-methods-to-measure-the-similarity-between-images-f907166694ee</em></p>
</section>
</section>
<section id="adversarial-attacks">
<h1>Adversarial Attacks<a class="headerlink" href="#adversarial-attacks" title="Link to this heading"></a></h1>
<p>AdvSecureNet supports various adversarial attacks, including:</p>
<ul class="simple">
<li><p>Fast Gradient Sign Method (FGSM)</p></li>
<li><p>Carlini and Wagner (C&amp;W)</p></li>
<li><p>Projected Gradient Descent (PGD)</p></li>
<li><p>DeepFool</p></li>
<li><p>Decision Boundary</p></li>
<li><p>Layerwise Origin-Target Synthesis (LOTS)</p></li>
</ul>
<p>Some of these attacks are targeted, while others are untargeted.
AdvSecureNet provides a simple way to use targeted adversarial attacks
by having an automatic target generation mechanism. This mechanism
generates a target label that is different from the original label of
the input image. The target label is chosen randomly from the set of
possible labels, excluding the original label. This ensures that the
attack is targeted, as the goal is to mislead the model into predicting
the target label instead of the correct one without being explicitly
specified by the user. This feature is particularly useful for the large
datasets where manually specifying target labels for each input image is
impractical. However, users can also specify the target label manually
if they would like to do so.</p>
<p>Below, we provide a brief overview of each adversarial attack supported
by AdvSecureNet, including its characteristics, purpose, and potential
applications.</p>
<section id="adversarial-attacks-overview">
<h2>Adversarial Attacks Overview<a class="headerlink" href="#adversarial-attacks-overview" title="Link to this heading"></a></h2>
<p>Adversarial attacks can be categorized in different ways. One way to
categorize them is based on the <strong>information</strong> they use. Broadly, these
attacks fall into two categories: <strong>white-box</strong> and <strong>black-box</strong>
attacks <a href="#id69"><span class="problematic" id="id2">[1]_</span></a> <a href="#id70"><span class="problematic" id="id3">[2]_</span></a>. White-box attacks necessitate access to the model’s
parameters, making them intrinsically reliant on detailed knowledge of
the model’s internals <a href="#id71"><span class="problematic" id="id4">[3]_</span></a> <a href="#id72"><span class="problematic" id="id5">[4]_</span></a>. In contrast, black-box attacks operate
without requiring access to the model’s parameters <a href="#id73"><span class="problematic" id="id6">[5]_</span></a> <a href="#id74"><span class="problematic" id="id7">[6]_</span></a>. Among
black-box attack methods, one prevalent approach involves training a
substitute model to exploit the transferability of adversarial
attacks <a href="#id75"><span class="problematic" id="id8">[7]_</span></a>, targeting the victim model indirectly <a href="#id76"><span class="problematic" id="id9">[8]_</span></a>. Additionally,
there are other black-box attack methods such as decision boundary
attacks <a class="footnote-reference brackets" href="#id32" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> and zeroth order optimization based attacks such as
ZOO <a class="footnote-reference brackets" href="#id33" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. These methods, distinct from the substitute model approach,
rely solely on the output of the model, further reinforcing their
classification as black-box attacks.</p>
<p>Adversarial attacks can also be differentiated based on the <strong>number of
steps</strong> involved in generating adversarial perturbations. This
categorization divides them into <strong>single-step</strong> and <strong>iterative</strong>
attacks <a class="footnote-reference brackets" href="#id34" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>. Single-step attacks are characterized by their speed, as
they require only one step to calculate the adversarial perturbation. On
the other hand, iterative attacks are more time-consuming, involving
multiple steps to incrementally compute the adversarial
perturbation <a class="footnote-reference brackets" href="#id35" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>.</p>
<p>Additionally, another classification of adversarial attacks hinges on
the <strong>objective of the attack</strong>. In this context, attacks are grouped
into <strong>targeted</strong> and <strong>untargeted</strong> categories <a class="footnote-reference brackets" href="#id36" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>. Targeted attacks
are designed with the specific goal of manipulating the model’s output
to a predetermined class. In contrast, untargeted attacks are aimed at
causing the model to incorrectly classify the input into any class,
provided it is not the correct one <a class="footnote-reference brackets" href="#id37" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>.</p>
<hr class="docutils" />
<section id="fgsm">
<h3>FGSM<a class="headerlink" href="#fgsm" title="Link to this heading"></a></h3>
<p>FGSM, short for Fast Gradient Sign Method, is a type of adversarial
attack that was introduced by Goodfellow et al. <a class="footnote-reference brackets" href="#id38" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a> in 2015. It is a
single-step, white box attack. Initially, the attack is designed as an
untargeted attack. However, it can be modified to be a targeted attack.
The idea of the FGSM attack is to compute the adversarial perturbation
by taking the sign of the gradient of the loss function with respect to
the input. The FGSM attack is a fast attack since it only requires one
step to compute the adversarial perturbation. This makes it a popular
attack in the adversarial robustness literature. However, it has been
shown that the FGSM attack is not effective against the adversarial
defenses. This is because the FGSM attack is a weak attack and it can be
easily defended by the adversarial defenses such as adversarial
training.</p>
<p>If the attack is untargeted, the formula tries to maximize the loss
function with respect to the input and correct label. If the attack is
targeted, the formula tries to minimize the loss function with respect
to the input and target label since the purpose of the targeted attack
is to get closer to the target label. The untargeted FGSM attack is
given in the equation below, and the targeted FGSM attack is given in
the subsequent equation.</p>
<div class="math notranslate nohighlight">
\[\text{adv}_x = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))\]</div>
<div class="math notranslate nohighlight">
\[\text{adv}_x = x - \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{adv}_x\)</span> is the adversarial image.</p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> is the original input image.</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the original label for <strong>untargeted</strong> attacks, or the
target label for <strong>targeted</strong> attacks.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a multiplier to ensure the perturbations are
small.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> represents the model parameters.</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\theta, x, y)\)</span> is the loss function used by the model.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="c-w">
<h3>C&amp;W<a class="headerlink" href="#c-w" title="Link to this heading"></a></h3>
<p>The Carlini and Wagner (C&amp;W) attack <a class="footnote-reference brackets" href="#id39" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a>, introduced by Nicholas
Carlini and David Wagner in 2017, is a sophisticated method of
adversarial attack aimed at machine learning models, particularly those
used in computer vision. As an iterative, white-box attack, it requires
access to the model’s architecture and parameters. The core of the C&amp;W
attack involves formulating and solving an optimization problem that
minimally perturbs the input image in a way that leads to incorrect
model predictions. This is done while maintaining the perturbations
imperceptible to the human eye, thus preserving the image’s visual
integrity. The optimization process often employs techniques like binary
search to find the smallest possible perturbation that can deceive the
model. The C&amp;W attack is versatile, capable of being deployed as both
untargeted and targeted attacks. The attack can also use different
distance metrics when computing the perturbation, such as the L0, L2,
and L-infinity norms. The choice of distance metric can affect the
attack’s effectiveness and the perturbation’s perceptibility.</p>
<p>The attack’s effectiveness lies in its ability to subtly manipulate the
input data, challenging the robustness and security of machine learning
models, and it has become a benchmark for testing the vulnerability of
these models to adversarial examples. However, the biggest drawback of
the C&amp;W attack is its computational complexity, which stems from the
iterative nature of the attack and the optimization problem that needs
to be solved. This makes the C&amp;W attack less practical for real-world
applications.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp; \text{minimize} \quad \|\delta\|_p + c \cdot f(x + \delta) \\
&amp; \text{such that} \quad x + \delta \in [0,1]^n
\end{aligned}\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> is the perturbation added to the input image
<span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\|\delta\|_p\)</span> is the p-norm of the perturbation, which
measures the size of the perturbation.</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> is a constant that balances the perturbation magnitude and
the success of the attack.</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x + \delta)\)</span> is the objective function, designed to mislead
the model into making incorrect predictions.</p></li>
<li><p><span class="math notranslate nohighlight">\(x + \delta \in [0,1]^n\)</span> ensures that the perturbed input
remains within the valid input range for the model.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="pgd">
<h3>PGD<a class="headerlink" href="#pgd" title="Link to this heading"></a></h3>
<p>The Projected Gradient Descent (PGD) attack <a class="footnote-reference brackets" href="#id40" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a> is a prominent
adversarial attack method in the field of machine learning, particularly
for evaluating the robustness of models against adversarial examples.
Introduced by Madry et al. <a class="footnote-reference brackets" href="#id41" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a>, the PGD attack is an iterative method
that generates adversarial examples by repeatedly applying a small
perturbation and projecting this perturbation onto an
<span class="math notranslate nohighlight">\(\varepsilon\)</span>-ball around the original input within a specified
norm. This process is repeated for a fixed number of steps or until a
successful adversarial example is found. The PGD attack operates under a
white-box setting, where the attacker has full knowledge of the model,
including its architecture and parameters. The strength of the PGD
attack lies in its simplicity and effectiveness in finding adversarial
examples within a constrained space, making it a standard benchmark in
adversarial robustness research. According to cite here, the PGD attack
is one of the most used attacks in adversarial training. However,
similar to other adversarial attacks like the C&amp;W attack, the PGD attack
can be computationally intensive, particularly when dealing with complex
models and high-dimensional input spaces, which may limit its
practicality in real-world scenarios.</p>
</section>
<hr class="docutils" />
<section id="deepfool">
<h3>DeepFool<a class="headerlink" href="#deepfool" title="Link to this heading"></a></h3>
<p>The DeepFool attack, introduced by Moosavi-Dezfooli et al. <a class="footnote-reference brackets" href="#id42" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a> in
2016, is a type of adversarial attack that aims to generate adversarial
examples that are close to the original input but mislead the model. It
is an iterative, white-box attack. The algorithm works by linearizing
the decision boundaries of the model and then applying a small
perturbation that pushes the input just across this boundary. This
process is repeated iteratively until the input is misclassified,
ensuring that the resulting adversarial example is as close to the
original input as possible. One of the key strengths of DeepFool is its
ability to compute these minimal perturbations with relatively low
computational overhead compared to other methods because of its
linearization approach. Despite its efficiency, the attack assumes a
somewhat idealized linear model, which may not always accurately reflect
the complex decision boundaries in more advanced, non-linear models.
Nonetheless, DeepFool has become a valuable tool in the adversarial
machine learning toolkit for its ability to provide insights into model
vulnerabilities with minimal perturbations.</p>
</section>
<hr class="docutils" />
<section id="decision-boundary">
<h3>Decision Boundary<a class="headerlink" href="#decision-boundary" title="Link to this heading"></a></h3>
<p>The Decision Boundary attack is a black-box attack that was introduced
by Brendel et al. <a class="footnote-reference brackets" href="#id43" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a> in 2017. The idea of the Decision Boundary
attack is to find the decision boundary of the model and then apply a
small perturbation that pushes the input just across this boundary. The
Decision Boundary attack is an iterative attack and can be both targeted
and untargeted. The attack starts with a random input that is initially
adversarial and then iteratively updates the input to get closer to the
decision boundary and minimize the perturbation. The advantage of the
attack is that it does not require any information about the model. This
makes it more suitable for real-world applications where the model’s
information is not available. However, the drawback of the attack is
that it is computationally expensive since it requires iteratively
updating the input to get closer to the decision boundary.</p>
</section>
<hr class="docutils" />
<section id="lots">
<h3>LOTS<a class="headerlink" href="#lots" title="Link to this heading"></a></h3>
<p>LOTS, Layerwise Origin-Target Synthesis <a class="footnote-reference brackets" href="#id44" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a>, is a type of adversarial
attack that was introduced by Rozsa et al. in 2017. It is a versatile,
white-box attack that can be used as both targeted and untargeted
attacks, single-step and iterative. The idea of the LOTS attack is to
compute the adversarial perturbation by using the deep feature layers of
the model. The purpose of the attack algorithm is to adjust the deep
feature representation of the input to match the deep feature
representation of the target class. Utilizing deep feature
representations makes the LOTS attack suitable for systems that use deep
feature representations, such as face recognition systems. The results
show that the Iterative LOTS attack is highly successful against the VGG
Face network with success rates between 98.28% to 100% <a class="footnote-reference brackets" href="#id45" id="id23" role="doc-noteref"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></a>. However,
the drawback of the LOTS attack is that it needs to know the deep
feature representation of the target class.</p>
</section>
<hr class="docutils" />
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Link to this heading"></a></h3>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id24" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id25" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., &amp;
Mukhopadhyay, D. (2018). Adversarial Attacks and Defences: A Survey.
arXiv preprint arXiv:1810.00069.</p>
</aside>
<aside class="footnote brackets" id="id26" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id27" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<p>Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., &amp;
Mukhopadhyay, D. (2018). Adversarial Attacks and Defences: A Survey.
arXiv preprint arXiv:1810.00069.</p>
</aside>
<aside class="footnote brackets" id="id28" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id29" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<p>Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., &amp;
Mukhopadhyay, D. (2018). Adversarial Attacks and Defences: A Survey.
arXiv preprint arXiv:1810.00069.</p>
</aside>
<aside class="footnote brackets" id="id30" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<p>Papernot, N., Mcdaniel, P., &amp; Goodfellow, I. J. (2016).
Transferability in Machine Learning: from Phenomena to Black-Box
Attacks using Adversarial Samples. arXiv preprint arXiv:1605.07277.
Retrieved from https://api.semanticscholar.org/CorpusID:17362994</p>
</aside>
<aside class="footnote brackets" id="id31" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<p>Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., &amp;
Swami, A. (2017). Practical Black-Box Attacks against Machine
Learning. arXiv preprint arXiv:1602.02697.</p>
</aside>
<aside class="footnote brackets" id="id32" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">9</a><span class="fn-bracket">]</span></span>
<p>Brendel, W., Rauber, J., &amp; Bethge, M. (2017). Decision-Based
Adversarial Attacks: Reliable Attacks Against Black-Box Machine
Learning Models. arXiv preprint arXiv:1712.04248. Retrieved from
https://api.semanticscholar.org/CorpusID:2410333</p>
</aside>
<aside class="footnote brackets" id="id33" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">10</a><span class="fn-bracket">]</span></span>
<p>Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., &amp; Hsieh, C.-J. (2017).
ZOO: Zeroth Order Optimization Based Black-Box Attacks to Deep Neural
Networks without Training Substitute Models. In <em>Proceedings of the
10th ACM Workshop on Artificial Intelligence and Security</em>
(pp. 15-26). New York, NY, USA: Association for Computing Machinery.
doi:
<a class="reference external" href="https://doi.org/10.1145/3128572.3140448">10.1145/3128572.3140448</a></p>
</aside>
<aside class="footnote brackets" id="id34" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">11</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id35" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">12</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id36" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">13</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id37" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">14</a><span class="fn-bracket">]</span></span>
<p>Khalid, F., Hanif, M. A., &amp; Shafique, M. (2021). Exploiting
Vulnerabilities in Deep Neural Networks: Adversarial and
Fault-Injection Attacks. arXiv preprint arXiv:2105.03251.</p>
</aside>
<aside class="footnote brackets" id="id38" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">15</a><span class="fn-bracket">]</span></span>
<p>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. (2015). Explaining and
Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572.</p>
</aside>
<aside class="footnote brackets" id="id39" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">16</a><span class="fn-bracket">]</span></span>
<p>Carlini, N., &amp; Wagner, D. (2017). “Towards Evaluating the Robustness
of Neural Networks.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1702.04267">arXiv:1702.04267</a></p>
</aside>
<aside class="footnote brackets" id="id40" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">17</a><span class="fn-bracket">]</span></span>
<p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2017).
“Towards Deep Learning Models Resistant to Adversarial Attacks.”
Available at: <a class="reference external" href="https://arxiv.org/abs/1706.06083">arXiv:1706.06083</a></p>
</aside>
<aside class="footnote brackets" id="id41" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">18</a><span class="fn-bracket">]</span></span>
<p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2017).
“Towards Deep Learning Models Resistant to Adversarial Attacks.”
Available at: <a class="reference external" href="https://arxiv.org/abs/1706.06083">arXiv:1706.06083</a></p>
</aside>
<aside class="footnote brackets" id="id42" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">19</a><span class="fn-bracket">]</span></span>
<p>Moosavi-Dezfooli, S.-M., Fawzi, A., &amp; Frossard, P. (2016). “DeepFool:
A Simple and Accurate Method to Fool Deep Neural Networks.” Available
at: <a class="reference external" href="https://arxiv.org/abs/1511.04599">arXiv:1511.04599</a></p>
</aside>
<aside class="footnote brackets" id="id43" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">20</a><span class="fn-bracket">]</span></span>
<p>Brendel, W., Rauber, J., &amp; Bethge, M. (2017). “Decision-Based
Adversarial Attacks: Reliable Attacks Against Black-Box Machine
Learning Models.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1712.04248">arXiv:1712.04248</a></p>
</aside>
<aside class="footnote brackets" id="id44" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">21</a><span class="fn-bracket">]</span></span>
<p>Rozsa, A., Rudd, E. M., &amp; Boult, T. E. (2017). “LOTS: Layerwise
Origin-Target Synthesis for Adversarial Attack.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1611.06503">arXiv:1611.06503</a></p>
</aside>
<aside class="footnote brackets" id="id45" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">22</a><span class="fn-bracket">]</span></span>
<p>Rozsa, A., Rudd, E. M., &amp; Boult, T. E. (2017). “LOTS: Layerwise
Origin-Target Synthesis for Adversarial Attack.” Available at:
<a class="reference external" href="https://arxiv.org/abs/1611.06503">arXiv:1611.06503</a></p>
</aside>
</aside>
</section>
</section>
</section>
<section id="adversarial-defenses">
<h1>Adversarial Defenses<a class="headerlink" href="#adversarial-defenses" title="Link to this heading"></a></h1>
<section id="adversarial-training">
<h2>Adversarial Training<a class="headerlink" href="#adversarial-training" title="Link to this heading"></a></h2>
<p>Adversarial training is a methodology in machine learning, particularly within the field of deep learning, aimed at improving the robustness and generalization of models against adversarial examples. These are inputs deliberately crafted to deceive models into making incorrect predictions or classifications <a href="#id77"><span class="problematic" id="id46">[1]_</span></a>. The concept of adversarial training emerged as a critical response to the observation that neural networks, despite their high accuracy, are often vulnerable to subtly modified inputs that are imperceptible to humans <a href="#id78"><span class="problematic" id="id47">[2]_</span></a>.</p>
<p>The core idea behind adversarial training involves the intentional generation of adversarial examples during the training process. By exposing the model to these challenging scenarios, the model learns to generalize better and becomes more resistant to such attacks <a href="#id79"><span class="problematic" id="id48">[3]_</span></a>. So far, adversarial training represents the only known defense that works to some extent and scale against adversarial attacks <a href="#id80"><span class="problematic" id="id49">[4]_</span></a>.</p>
<p><img alt="adversarial_training" src="https://github.com/melihcatal/advsecurenet/assets/46859098/da22a465-03b5-4583-97ac-fded10999073" /> <em>Adversarial Training Flow</em>
<img alt="adversarial_data_generator" src="https://github.com/melihcatal/advsecurenet/assets/46859098/4ac0a8cb-a97f-4363-b05e-06444f7cd0f5" /> <em>Adversarial Data Generator</em></p>
</section>
<section id="ensemble-adversarial-training">
<h2>Ensemble Adversarial Training<a class="headerlink" href="#ensemble-adversarial-training" title="Link to this heading"></a></h2>
<p>Ensemble Adversarial Training, proposed by Florian Tramèr et al. <a href="#id81"><span class="problematic" id="id50">[5]_</span></a> in 2018, is a type of adversarial training that aims to improve the robustness of the model to unseen attacks and black-box attacks by generalizing the adversarial training process. The idea of the ensemble adversarial training is crafting adversarial examples from a set of pretrained substitute models in addition to the adversarial examples crafted from the original source model that the defender wants to robustify. The intuition is that crafting adversarial samples only from the source model can lead to overfitting to the source model and the model still can be vulnerable to unseen attacks and black-box attacks. However, crafting adversarial samples from a set of pretrained substitute models can lead to generalization of the adversarial training process and improve the robustness of the model to unseen attacks and black-box attacks. The experiments <a href="#id82"><span class="problematic" id="id51">[5]_</span></a> showed that the ensemble adversarial training can improve the robustness of the model to unseen attacks and black-box attacks but lower the accuracy on clean examples.</p>
<p>The ensemble feature of the ensemble adversarial training refers to ensemble of models. However, it is also possible to ensemble the adversarial attacks <a href="#id83"><span class="problematic" id="id52">[5]_</span></a>. The intuition is similar to the ensemble of models, which is generalization since the adversarial training does not offer a guarantee to the unseen attacks <a href="#id84"><span class="problematic" id="id53">[6]_</span></a> <a href="#id85"><span class="problematic" id="id54">[7]_</span></a>. It’s also shown that having a robust model to one type of attack can make the model more vulnerable to other types of attacks <a href="#id86"><span class="problematic" id="id55">[7]_</span></a> <a href="#id87"><span class="problematic" id="id56">[8]_</span></a>.</p>
<p>The ensemble of adversarial attacks refers to crafting adversarial examples from a set of adversarial attacks in addition to the adversarial examples crafted from the original adversarial attack. The purpose is having a robust model to different types of perturbations simultaneously <a href="#id88"><span class="problematic" id="id57">[5]_</span></a>. However, the results show that the models trained with ensemble of adversarial attacks are not robust as the models trained with each attack individually <a href="#id89"><span class="problematic" id="id58">[5]_</span></a>.</p>
<p><img alt="ensemble_adversarial_generator" src="https://github.com/melihcatal/advsecurenet/assets/46859098/23601123-2566-468b-b0c5-b1eaa0bf8069" />  <em>Ensemble Adversarial Generator. The generator randomly picks one model from the models pool and one attack from the attacks pool. Only having origin model and one attack is the same as classical adversarial training. Having one attack but multiple pretrained models is the Ensemble Adversarial Training. It’s also possible have ensemble models and ensemble attacks at the same time.</em></p>
</section>
<section id="id59">
<h2>References<a class="headerlink" href="#id59" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. (2015). Explaining and Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572.</p>
</aside>
<aside class="footnote brackets" id="id61" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., &amp; Fergus, R. (2013). Intriguing properties of neural networks. CoRR, abs/1312.6199. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:604334">https://api.semanticscholar.org/CorpusID:604334</a></p>
</aside>
<aside class="footnote brackets" id="id62" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2017). Towards Deep Learning Models Resistant to Adversarial Attacks. ArXiv, abs/1706.06083. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:3488815">https://api.semanticscholar.org/CorpusID:3488815</a></p>
</aside>
<aside class="footnote brackets" id="id63" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<p>Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., Madry, A., &amp; Kurakin, A. (2019). On Evaluating Adversarial Robustness. arXiv preprint arXiv:1902.06705.</p>
</aside>
<aside class="footnote brackets" id="id64" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<p>Tramer, F., Kurakin, A., Papernot, N., Boneh, D., &amp; Mcdaniel, P. (2017). Ensemble Adversarial Training: Attacks and Defenses. ArXiv, abs/1705.07204. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:21946795">https://api.semanticscholar.org/CorpusID:21946795</a></p>
</aside>
<aside class="footnote brackets" id="id65" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<p>Papernot, N., Mcdaniel, P., &amp; Goodfellow, I. J. (2016). Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. ArXiv, abs/1605.07277. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:17362994">https://api.semanticscholar.org/CorpusID:17362994</a></p>
</aside>
<aside class="footnote brackets" id="id66" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<p>Schott, L., Rauber, J., Bethge, M., &amp; Brendel, W. (2018). Towards the first adversarially robust neural network model on MNIST. arXiv preprint arXiv:1805.09190.</p>
</aside>
<aside class="footnote brackets" id="id67" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<p>Engstrom, L., Tsipras, D., Schmidt, L., &amp; Madry, A. (2017). A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. ArXiv, abs/1712.02779. Available at: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:21929206">https://api.semanticscholar.org/CorpusID:21929206</a></p>
</aside>
</aside>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">AdvSecureNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="attacks.html">Adversarial Attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="defenses.html">Adversarial Defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluations.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
</ul>
</div>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction.html" class="btn btn-neutral float-right" title="AdvSecureNet" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Melih Catal.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>