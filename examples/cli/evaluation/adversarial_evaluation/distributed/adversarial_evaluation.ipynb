{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Adversarial Evaluation\n",
    "\n",
    "Adversarial Evaluation is a key component of the robustness evaluation of machine learning models. There are different ways to evaluate the robustness of a model, efficiency of a defense mechanism or the adversarial attack itself. `advsecurenet` supports a variety of adversarial evaluation techniques. Such evaluations might be time consuming by their nature. Therefore, `advsecurenet` provides a distributed adversarial evaluation option to speed up the evaluation process. In this notebook, we will demonstrate how to use distributed adversarial evaluation in `advsecurenet` CLI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Configuration File\n",
    "Due to the complexity of the adversarial evaluation process, the only way to use adversarial evaluation on CLI is to use a configuration file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Configuration File\n",
    "\n",
    "As a first step, you need to have a default configuration file. You can create a configuration file by running the following command:\n",
    "    \n",
    "```sh\n",
    "advsecurenet utils configs get -c adversarial_evaluation_config.yml -s -o ./adversarial_evaluation_config.yml\n",
    "```\n",
    "\n",
    "Here, we are also providing the `-o` flag to specify the output directory where the configuration file will be saved. You can also specify the output directory by passing the `-o` flag followed by the path to the directory where you want to save the configuration file. If you don't provide the `-o` flag, the configuration file will be saved in the current working directory with the name `adversarial_evaluation_config.yml`. If the file already exists, it will be overwritten. If the output path is a directory, the configuration file will be saved in that directory with the name `adversarial_evaluation_config.yml`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!advsecurenet utils configs get -c adversarial_evaluation_config.yml -s -o ./adversarial_evaluation_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!advsecurenet utils configs get -c adversarial_evaluation_config.yml -p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation configuration file might look like less complex than the other configuration files. However, it depends on external configurations such as the attack configuration file and the model configuration file. In this notebook, we will use the `FGSM` attack and as evaluators, we will use `attack_success_rate` and `transferability` evaluators. To evaluate the transferability, we need to have a second model. Therefore, we will use the `ResNet18` as the source model and the `VGG16` as the target model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get Required Configuration Files\n",
    "\n",
    "To get the required configuration files, you can run the following commands:\n",
    "\n",
    "```sh\n",
    "advsecurenet utils configs get -c fgsm_attack_config.yml -s -o ./fgsm_attack_config.yml\n",
    "advsecurenet utils configs get -c model_config.yml -s -o ./model_config.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!advsecurenet utils configs get -c fgsm_attack_config.yml -s -o ./fgsm_attack_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!advsecurenet utils configs get -c model_config.yml -s -o ./target_model_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Update the Configuration File\n",
    "For the sake of this demonstration, we will update the configuration files in place, we won't create new configuration files.\n",
    "\n",
    "The difference between the distributed adversarial evaluation configuration file and the regular adversarial evaluation configuration file is the `use_ddp` key in the **attack configuration file**. If you set the `use_ddp` key to `true`, the adversarial evaluation process will be distributed. \n",
    "\n",
    "```yaml\n",
    "device:\n",
    "  use_ddp: true\n",
    "  processor: cuda\n",
    "  gpu_ids: [0, 1, 2, 3]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Run the Adversarial Evaluation\n",
    "\n",
    "Now, we can run the adversarial evaluation by running the following command:\n",
    "\n",
    "```sh\n",
    "advsecurenet evaluate adversarial eval -c ./adversarial_evaluation_config.yml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!advsecurenet evaluate adversarial eval -c ./adversarial_evaluation_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully run the adversarial evaluation in `advsecurenet` CLI. You can try different configurations and models to evaluate the robustness of your models, efficiency of your defense mechanisms, or the adversarial attacks themselves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
